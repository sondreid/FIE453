return (model)
}
build_nn_model_3_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 8,  activation = "relu")
model <- keras_model(input, output)
return(model)
}
build_nn_model_1_layer <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu")
model <- keras_model(input, output)
return(model)
}
nn_model_1_layer_reduced <- build_nn_model_1_layer(train_df_reduced, spec_reduced)
spec_reduced
library(tensorflow)
library(keras)
library(tfdatasets)
library(reticulate)
set_random_seed (42, disable_gpu = FALSE) # Set seed for reproducability, both tensorflow and R native seed
conda_python(envname = "r-reticulate") # Create miniconda enviroment (if not already done)
tensorflow::use_condaenv("r-reticulate") # Specify enviroment to tensorflow
################################################################################
########################## Train and Test Split ################################
################################################################################
######### Dataframe with all companies using only variance and correlation filter#############
# Load all
load(file = "cached_data/train_test.Rdata")
# Or run the code in preprocessing
test_df_reduced$vwretd
make_0_benchmark <- function(selected_test_df) {
#' Makes a zero benchmark to compare models
#' @
benchmark_0 <- postResample(rep(0, nrow(selected_test_df)), selected_test_df$retx)
return (benchmark_0)
}
# Neural network using specified number of layers  ---------------------------------------
spec <- feature_spec(train_df, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scaler_standard()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
spec_reduced <- feature_spec(train_df_reduced, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scaler_standard()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
print_dot_callback <- callback_lambda(
#' Simplified callback, showing dots instead of full loss/validation error plots
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat("\n")
cat(".")
}
)
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
build_nn_model_5_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 8, activation = "relu") %>%
layer_dense(units = 4,  activation = "relu") %>%
layer_dense(units = 2,  activation = "relu")
model <- keras_model(input, output)
return (model)
}
build_nn_model_3_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 8,  activation = "relu")
model <- keras_model(input, output)
return(model)
}
build_nn_model_1_layer <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu")
model <- keras_model(input, output)
return(model)
}
nn_model_1_layer_reduced <- build_nn_model_1_layer(train_df_reduced, spec_reduced)
nn_model_3_layers_reduced <- build_nn_model_3_layers(train_df_reduced, spec_reduced)
nn_model_5_layers_reduced <- build_nn_model_5_layers(train_df_reduced, spec_reduced)
build_nn_model_3_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dense(units = 8,  activation = "relu") %>%
layer_batch_normalization()
model <- keras_model(input, output)
return(model)
}
build_nn_model_1_layer <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu")
model <- keras_model(input, output)
return(model)
}
nn_model_1_layer_reduced <- build_nn_model_1_layer(train_df_reduced, spec_reduced)
nn_model_3_layers_reduced <- build_nn_model_3_layers(train_df_reduced, spec_reduced)
adam_opt = optimizer_adam()
best_model_nn_3_layer_adam <- grid_search_nn_model_generaL_optimizer(nn_model_3_layers_reduced, train_df_reduced, test_df_reduced,
batch_sizes = list(100, 500, 1500, 7000),
epochs = list(1500),
optimizer = adam_opt,
patience_list = list(5,10, 15, 25, 50),
verbose = 0
)
grid_search_nn_model <- function(selected_model, model_train_df, model_test_df, learning_rates, momentums,
epochs, batch_sizes,
patience_list, verbose) {
best_MAE <- Inf
best_model <- NA
best_history <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
for (epoch in epochs) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
new_early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience)
new_model <- selected_model %>%
compile(
loss = "mse",
optimizer = optimizer_sgd(
learning_rate =lr,
momentum = momentum),
metrics = list("mean_absolute_error"))
history <- new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_history <- history
best_model <- new_model
best_MAE <- mae
}
}
}
}
}
}
return (list(best_model, best_history))
}
grid_search_nn_model_generaL_optimizer <- function(selected_model, model_train_df, model_test_df,
epochs, batch_sizes, optimizer, patience_list,  verbose) {
best_MAE <- Inf
best_model <- NA
best_history <- NA
for (epoch in epochs) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
new_early_stop <- callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_model <- selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
history <- new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_history <- history
best_model <- new_model
best_MAE <- mae
}
}
}
}
return (list(best_model, best_history))
}
best_model_nn_3_layer_adam <- grid_search_nn_model_generaL_optimizer(nn_model_3_layers_reduced, train_df_reduced, test_df_reduced,
batch_sizes = list(100, 500, 1500, 7000),
epochs = list(1500),
optimizer = adam_opt,
patience_list = list(5,10, 15, 25, 50),
verbose = 0
)
### REDUCED DATA SET FOR TESTING
selection_data_reduced <- get_subset_of_companies_ratio(selection_data, 0.1)
# Train-Test-Split
train_test_reduced <- perform_train_test_split(selection_data_reduced,
train_ratio = 0.8)                       # Split into train and test set with seperate sets of companies
train_df_reduced <- train_test_reduced[[1]]
test_df_reduced <- train_test_reduced[[2]]
train_df_reduced %<>% dplyr::select(-permno) # Remove company numbers from training
low_observation_count_companies <- find_company_observations(test_df_reduced, 50)
test_df_reduced %<>% anti_join(low_observation_count_companies)                        # Cut companies with fewer than 50 observations (they cannot be reliably predicted)
### SAVE datasets
save(train_df, selection_data, test_df, train_df_reduced, test_df_reduced, file = "cached_data/train_test.Rdata")
######################## DATA PROCESSING FUNCTIONS  ##########################
get_company_name <- function(input_permno) {
#'
#'@description: Returns the name of a company based on its company identification number
company_name <- company_names_df %>%
filter(permno == input_permno)
# If several names are registered. Pick the most recent
company_name %<>% arrange(desc(date))
return( company_name$comnam[1])
}
# Data reduction ---------------------------------------------------------------
get_subset_of_companies <-function(df, number_of_companies) {
#' @description:              To reduce run time, we want to reduce the
#'                            number of companies,(for variable selection
#'                            purposes)
#'
#' @param df                  The dataframe to be split
#' @param number_of_companies The number of speakers to be retained
#' @return                    A dataframe of fewer companies
set.seed(123)
companies <- df$permno %>% unique()
subset_of_companies <- companies %>%
sample(x = ., size = number_of_companies)
return(df %>% filter(permno %in% subset_of_companies))
}
get_subset_of_companies_ratio <-function(df, ratio) {
#' @Description:              To reduce run time, we want to reduce the
#'                            number of companies, (for variable selection
#'                            purposes)
#'
#' @param df                  The dataframe to be split
#' @param number_of_companies The number of speakers to be retained
#' @return:                   A dataframe of fewer companies
set.seed(123)
companies <- df$permno %>% unique()
number_of_companies <- companies %>% length()
subset_of_companies <- companies %>%
sample(x = ., size = as.integer(number_of_companies*ratio))
return(df %>% filter(permno %in% subset_of_companies))
}
# Feature selection functions --------------------------------------------------
remove_cols_only_zero_and_NA <- function(df, print_removed_cols = F) {
#' @description              Function that removes columns containing only
#'                           zeros and NAs
#'
#' @param df                 Passing a data frame
#' @param print_removed_cols True if user want to print removed columns
#' @return                   Data frame without columns that containing only
#'                           zeros and NAs
cols <- df %>%
apply(MARGIN = 2,
function(x) (sum(x==0, na.rm = T) + sum(is.na(x)))/length(x))
cols <- cols[cols == 1] %>%
as.data.frame() %>%
rownames()
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return (df %>% dplyr::select(-cols))
}
remove_NA <- function(df, ratio, print_removed_cols = F){
#' @description              Function that removes columns containing NAs
#'                           beyond a given ratio
#'
#' @param df                 Passing a data frame
#' @param ratio              Passing a upper limit NA ratio
#' @param print_removed_cols True if user want to print removed columns
#' @return                   Data frame without columns containing NAs
#'                           beyond given ratio
cols <- df %>%
apply(MARGIN = 2,
function(x) sum(is.na(x))/length(x))
cols <- cols[cols >= ratio] %>%
as.data.frame() %>%
rownames()
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% dplyr::select(-cols))
}
remove_nzv <- function(df, print_removed_cols = F){
#' @description              Function that removes near zero variance
#'                           columns
#'
#' @param df                 Passing a data frame
#' @param print_removed_cols True if user want to print removed columns
#' @return                   Data frame without columns near zero variance
#'                           columns
rec <- recipe(retx ~ .,
data = df)
cols <- (rec %>%
step_nzv(all_predictors()) %>%
prep(df) %>%
tidy(number = 1))$terms
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% dplyr::select(-cols))
}
remove_hcv <- function(df, threshold = 0.8, print_removed_cols = F){
#' @description              Function that removes highly correlated
#'                           features
#'
#' @param df                 Passing a data frame
#' @param treshold           Correlation beneath this threshold
#' @param print_removed_cols True if user want to print removed columns
#' @return                   Data frame without highly correlated features
numeric_cols <- df %>%
lapply(is.numeric) %>%
unlist()
rec <- recipe(retx ~ .,
data = df[numeric_cols])
cols <- (rec %>%
step_corr(all_predictors(),
threshold = threshold) %>%
prep(df[numeric_cols]) %>%
tidy(number = 1))$terms
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% dplyr::select(-cols))
}
replace_NA_with_mean <- function(df, print_replaced_cols = F){
#' @description               Function that replaces NA with column means
#'
#' @param df                  Passing a data frame
#' @param print_replaced_cols True if user want to print replaced columns
#' @return                    Data frame NA-replaced column means
na_cols <- df %>%
apply(MARGIN = 2,
function(x) any(is.na(x)))
numeric_cols <- df[na_cols] %>%
lapply(is.numeric) %>%
unlist()
col_means <- df[na_cols] %>%
colMeans(na.rm = T)
col_names <- col_means %>%
names()
for (col in col_names){
df[col] <- df[col][[1]] %>%
replace_na(col_means[col])
}
if(print_replaced_cols) cat("Columns replaced: ", col_names, "\n\n")
return(df)
}
remove_NA_rows <- function(df) {
#' @description Function that removes any rows with one or more NA's
#'
#' @param df    Passing a data frame
#' @return      Data frame NA filtered rows
return(df %>% filter(across(everything(), ~ !is.na(.x))) )
}
perform_train_test_split <- function(df, train_ratio = 0.8) {
#' @description Ensures an equal amount of companies in each set
#'
#' @param df    The dataframe to be split
#' @param ratio Ratio of training data, (validation and test set to equal
#'              length)
#' @return      A list of three data frames: training, validation and
#'              test sets
set.seed(123)
all_companies <- df$permno %>% unique()
train_indices <- sample(1:length(all_companies),
floor(length(all_companies) * train_ratio))
train_companies <- all_companies[train_indices]
test_companies <- all_companies[-train_indices]
train_sample <- df %>% filter(permno %in% train_companies)
test_sample  <- df %>% filter(permno %in% test_companies)
return (list(train_sample, test_sample))
}
find_company_observations <- function(df, minimum_observations) {
#' @description                 Finds companies that have less than a
#'                              minimum amount of observations
#'
#' @param df                    Passing a data frame
#' @param minimum_observations  Passing minimum observation limit
#' @return                      A data frame
all_companies <- df$permno %>%
unique()
df %<>% group_by(permno) %>%
summarise(count = n()) %>%
ungroup() %>%
filter(count < minimum_observations) %>%
arrange(desc(count))
return(df)
}
### REDUCED DATA SET FOR TESTING
selection_data_reduced <- get_subset_of_companies_ratio(selection_data, 0.1)
# Train-Test-Split
train_test_reduced <- perform_train_test_split(selection_data_reduced,
train_ratio = 0.8)                       # Split into train and test set with seperate sets of companies
train_df_reduced <- train_test_reduced[[1]]
test_df_reduced <- train_test_reduced[[2]]
train_df_reduced %<>% dplyr::select(-permno) # Remove company numbers from training
low_observation_count_companies <- find_company_observations(test_df_reduced, 50)
test_df_reduced %<>% anti_join(low_observation_count_companies)                        # Cut companies with fewer than 50 observations (they cannot be reliably predicted)
### SAVE datasets
save(train_df, selection_data, test_df, train_df_reduced, test_df_reduced, file = "cached_data/train_test.Rdata")
library(magrittr)
selection_data_reduced <- get_subset_of_companies_ratio(selection_data, 0.1)
# Train-Test-Split
train_test_reduced <- perform_train_test_split(selection_data_reduced,
train_ratio = 0.8)                       # Split into train and test set with seperate sets of companies
train_df_reduced <- train_test_reduced[[1]]
test_df_reduced <- train_test_reduced[[2]]
train_df_reduced %<>% dplyr::select(-permno) # Remove company numbers from training
low_observation_count_companies <- find_company_observations(test_df_reduced, 50)
test_df_reduced %<>% anti_join(low_observation_count_companies)                        # Cut companies with fewer than 50 observations (they cannot be reliably predicted)
# Libraries --------------------------------------------------------------------
library(tidyverse)
library(magrittr)
library(tidymodels)
library(randomForest)
library(caret)
library(doParallel)
library(MLmetrics)
library(gbm)
library(PerformanceAnalytics)
library(kableExtra)
library(knitr)
library(monomvn)
library(kableExtra)
library(lubridate)
library(kknn)
library(nnet)
selection_data_reduced <- get_subset_of_companies_ratio(selection_data, 0.1)
# Train-Test-Split
train_test_reduced <- perform_train_test_split(selection_data_reduced,
train_ratio = 0.8)                       # Split into train and test set with seperate sets of companies
train_df_reduced <- train_test_reduced[[1]]
test_df_reduced <- train_test_reduced[[2]]
train_df_reduced %<>% dplyr::select(-permno) # Remove company numbers from training
low_observation_count_companies <- find_company_observations(test_df_reduced, 50)
test_df_reduced %<>% anti_join(low_observation_count_companies)                        # Cut companies with fewer than 50 observations (they cannot be reliably predicted)
### SAVE datasets
save(train_df, selection_data, test_df, train_df_reduced, test_df_reduced, file = "cached_data/train_test.Rdata")
spec_reduced <- feature_spec(train_df_reduced, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scaler_standard()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
print_dot_callback <- callback_lambda(
#' Simplified callback, showing dots instead of full loss/validation error plots
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat("\n")
cat(".")
}
)
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
build_nn_model_5_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 8, activation = "relu") %>%
layer_dense(units = 4,  activation = "relu") %>%
layer_dense(units = 2,  activation = "relu")
model <- keras_model(input, output)
return (model)
}
build_nn_model_3_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dense(units = 8,  activation = "relu") %>%
layer_batch_normalization()
model <- keras_model(input, output)
return(model)
}
build_nn_model_3_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32) %>%
layer_batch_normalization() %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 32) %>%
layer_batch_normalization() %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 32) %>%
layer_batch_normalization() %>%
layer_activation(activation = "relu") %>%
model <- keras_model(input, output)
return(model)
}
build_nn_model_1_layer <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu")
model <- keras_model(input, output)
return(model)
}
nn_model_1_layer_reduced <- build_nn_model_1_layer(train_df_reduced, spec_reduced)
nn_model_3_layers_reduced <- build_nn_model_3_layers(train_df_reduced, spec_reduced)
nn_model_5_layers_reduced <- build_nn_model_5_layers(train_df_reduced, spec_reduced)
