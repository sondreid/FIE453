model_evaluation %>% arrange(desc('Test*MAE'))
model_evaluation %>% arrange(desc(`Test*MAE`))
model_evaluation %>% arrange(desc(`Test MAE`))
model_evaluation %>% arrange((`Test MAE`)
model_evaluation %>% arrange(`Test MAE`)
model_evaluation %>% arrange(`Test MAE`)
model_evaluation %<>% arrange(`Test MAE`)
model_evaluation
save(model_evaluation, file = "model_results/model_evalution.Rdata")
model_evaluation %>%
kable(caption = "Performance metrics of tested models",
digits  = 3) %>%
kable_classic(full_width = F,
html_font  = "Times New Roman")  %>%
save_kable("images/evaluation_metrics_all_models.png",
zoom = 1.5,
density = 1000)
selected_model <- model_evaluation["Test MAE"][1]
selected_model
summary(nn_model)
nn_model
model_evaluation
model_evaluation %>%
kable(caption = "Performance metrics of tested models",
digits  = 3) %>%
kable_classic(full_width = F,
html_font  = "Times New Roman")
selected_model <- nn_model
select_stocks <- function(test_df, selected_model) {
#' @description:         Function that selects stocks based on predictability
#'                       with their performance metrics
#'
#' @param test_df        Passing a test data frame
#' @param selected_model Passing a selected model
#' @return               Companies with highest predictability
companies <- test_df$permno %>% unique()
#test_df  %<>% left_join(company_names_df, by = "permno") # merge with company names
company_predictability <- tibble()
for (company in companies) {
company_data <- test_df %>%
filter(permno == company)
company_predictions <- predict(selected_model, company_data)
company_performance_metrics <- postResample(pred = company_predictions,
obs = test_df$retx)
company_predictability %<>% bind_rows(
tibble("Company name"       = get_company_name(company_data$permno[1]),
"Company identifier" = company_data$permno[1],
"Test RMSE"          = company_performance_metrics[[1]],
"Test MAE"           = company_performance_metrics[[3]])
)
}
return (company_predictability)
}
selected_stocks <- select_stocks(test_df, selected_model)
selected_stocks
View(selected_stocks)
feature_selection <- function() {
# Random Forest ----------------------------------------------------------------
set.seed(1)
mtry <- round(sqrt(ncol(train_df_reduced)))
tunegrid_rf <- expand.grid(.mtry = 2)
rf <- train(retx~.,
data       = train_df_reduced,
method     = "rf",
importance = TRUE,
metric     = "MAE",
preProcess = c("center","scale"),
tuneGrid   = tunegrid_rf,
trControl  = train_control)
rf$results$MAE %>% min() # Validation MAE
# Most important features according to RF model
var_importance_rf <- varImp(rf, scale = F)
var_importance_rf
# Test performance
rf_predictions <- predict(rf, test_df_reduced)
postResample(pred = rf_predictions, obs = test_df_reduced$retx)
# GBM --------------------------------------------------------------------------
tunegrid_gbm <-  expand.grid(interaction.depth = c(1, 5, 9),
n.trees = (1:30) * 50,
shrinkage = c(0.1, 0.2),
n.minobsinnode = c(5,10,20))
gbm <- train(retx~.,
data       = train_df_reduced,
method     = "gbm",
metric     = "MAE",
preProcess = c("center","scale"),
tuneGrid   = tunegrid_gbm,
trControl  = train_control)
gbm$results$MAE %>% min() # Validation MAE
# Test performance
gbm_predictions <- predict(gbm, test_df_reduced)
postResample(pred = gbm_predictions, obs = test_df_reduced$retx)
# Most important features according to gradient boosting model
var_importance_gbm <- varImp(gbm, scale = T)
if (postResample(pred = gbm_predictions, obs = test_df_reduced$retx)[[3]] < postResample(pred = rf_predictions, obs = test_df_reduced$retx)[[1]] ) {
# Store most important features
print("> GBM model used for feature selection")
most_important_features <- var_importance_gbm
}
else  most_important_features <- var_importance_rf
most_important_features <-
tibble(features = var_importance_gbm$importance %>%
as.data.frame() %>% row.names(),
score = var_importance_gbm$importance) %>%
arrange(desc(score$Overall))
# Saving
save(rf_predictions, gbm_predictions, most_important_features, file = "model_results/features.Rdata")
}
View(selected_stocks)
selected_model <- nn_model
select_stocks <- function(test_df, selected_model) {
#' @description:         Function that selects stocks based on predictability
#'                       with their performance metrics
#'
#' @param test_df        Passing a test data frame
#' @param selected_model Passing a selected model
#' @return               Companies with highest predictability
companies <- test_df$permno %>% unique()
#test_df  %<>% left_join(company_names_df, by = "permno") # merge with company names
company_predictability <- tibble()
for (company in companies) {
cat("Company", company)
company_data <- test_df %>%
filter(permno == company)
cat("Company data", company_data)
company_predictions <- predict(selected_model, company_data)
company_performance_metrics <- postResample(pred = company_predictions,
obs = test_df$retx)
company_predictability %<>% bind_rows(
tibble("Company name"       = get_company_name(company_data$permno[1]),
"Company identifier" = company_data$permno[1],
"Test RMSE"          = company_performance_metrics[[1]],
"Test MAE"           = company_performance_metrics[[3]])
)
}
return (company_predictability)
}
selected_stocks <- select_stocks(test_df, selected_model)
select_stocks <- function(test_df, selected_model) {
#' @description:         Function that selects stocks based on predictability
#'                       with their performance metrics
#'
#' @param test_df        Passing a test data frame
#' @param selected_model Passing a selected model
#' @return               Companies with highest predictability
companies <- test_df$permno %>% unique()
#test_df  %<>% left_join(company_names_df, by = "permno") # merge with company names
company_predictability <- tibble()
for (company in companies) {
cat("Company", company)
company_data <- test_df %>%
filter(permno == company)
print(paste("Company data", company_data))
company_predictions <- predict(selected_model, company_data)
company_performance_metrics <- postResample(pred = company_predictions,
obs = test_df$retx)
company_predictability %<>% bind_rows(
tibble("Company name"       = get_company_name(company_data$permno[1]),
"Company identifier" = company_data$permno[1],
"Test RMSE"          = company_performance_metrics[[1]],
"Test MAE"           = company_performance_metrics[[3]])
)
}
return (company_predictability)
}
selected_stocks <- select_stocks(test_df, selected_model)
test_df$permno
test_company <- test_df %>% filter(permno == 93436
)
test_company
predict(nn_model, test_company)
predict(knn_model, test_company)
selected_model <- knn_model
select_stocks <- function(test_df, selected_model) {
#' @description:         Function that selects stocks based on predictability
#'                       with their performance metrics
#'
#' @param test_df        Passing a test data frame
#' @param selected_model Passing a selected model
#' @return               Companies with highest predictability
companies <- test_df$permno %>% unique()
#test_df  %<>% left_join(company_names_df, by = "permno") # merge with company names
company_predictability <- tibble()
for (company in companies) {
company_data <- test_df %>%
filter(permno == company)
company_predictions <- predict(selected_model, company_data)
company_performance_metrics <- postResample(pred = company_predictions,
obs = test_df$retx)
company_predictability %<>% bind_rows(
tibble("Company name"       = get_company_name(company_data$permno[1]),
"Company identifier" = company_data$permno[1],
"Test RMSE"          = company_performance_metrics[[1]],
"Test MAE"           = company_performance_metrics[[3]])
)
}
return (company_predictability)
}
selected_stocks <- select_stocks(test_df, selected_model)
View(selected_stocks)
tunegrid_nn <-  expand.grid(size  = c(30, 50, 100),
decay = c(0.001, 0.05, 0.1))
# Training the NN-model
nn_model <- train(retx ~ .,
data       = train_df,
preProcess = c("center", "scale"),
trControl  = train_control,
tunegrid   = tunegrid_nn,
metric     = "MAE",
verbose = T,
allowParalell = T,
method     = "nnet")
nn_model
summary(nn_model)
tunegrid_nn <-  expand.grid(size  = c(5, 20, 70),
decay = c(0.001, 0.05, 0.1))
# Training the NN-model
nn_model <- train(retx ~ .,
data       = train_df,
preProcess = c("center", "scale"),
trControl  = train_control,
tuneGrid   = tunegrid_nn,
metric     = "MAE",
verbose = T,
allowParalell = T,
method     = "nnet")
nn_model
summary(nn_model)
evaluate_models <- function(modelList, train_df, test_df) {
#' @description     Function that evaluates the model both on the training set
#'                  and the test set by returning RMSE and MAE
#'
#' @param modelList Passing a list with fitted models
#' @param test_df   Passing test data frame
#' @return          Returns a tibble of test and validation metrics
model_performance <- tibble()
for (model in modelList) {
test_predictions          <- predict(model, newdata = test_df)
train_predictions         <- predict(model, newdata = train_df)
test_performance_metrics  <- postResample(pred = test_predictions,
obs = test_df$retx)
train_performance_metrics <- postResample(pred = train_predictions,
obs = train_df$retx)
model_performance %<>% bind_rows(
tibble(
"Model name"    =  model$method,
"Training RMSE" = train_performance_metrics[[1]],
"Training MAE"  = train_performance_metrics[[3]],
"Test RMSE"     = test_performance_metrics[[1]],
"Test MAE"      = test_performance_metrics[[3]]
)
)
}
return (model_performance)
}
modelList <- list(knn_model, nn_model, gam_model, bayesian_ridge_model)   # List of all models
model_evaluation <- evaluate_models(modelList, train_df,  test_df)  %>%  arrange("Test MAE")
model_evaluation
# Saving the models ------------------------------------------------------------
#save(knn_model, svm_model, gbm_model, file = "model_results/models.Rdata")
save(knn_model,nn_model, gam_model, bayesian_ridge_model, file = "models/models.Rdata")
save(model_evaluation, file = "model_results/model_evalution.Rdata")
selected_model <- bayesian_ridge_model
select_stocks <- function(test_df, selected_model) {
#' @description:         Function that selects stocks based on predictability
#'                       with their performance metrics
#'
#' @param test_df        Passing a test data frame
#' @param selected_model Passing a selected model
#' @return               Companies with highest predictability
companies <- test_df$permno %>% unique()
#test_df  %<>% left_join(company_names_df, by = "permno") # merge with company names
company_predictability <- tibble()
for (company in companies) {
company_data <- test_df %>%
filter(permno == company)
company_predictions <- predict(selected_model, company_data)
company_performance_metrics <- postResample(pred = company_predictions,
obs = test_df$retx)
company_predictability %<>% bind_rows(
tibble("Company name"       = get_company_name(company_data$permno[1]),
"Company identifier" = company_data$permno[1],
"Test RMSE"          = company_performance_metrics[[1]],
"Test MAE"           = company_performance_metrics[[3]])
)
}
return (company_predictability)
}
selected_stocks <- select_stocks(test_df, selected_model)
View(selected_model)
# Stop cluster
stopCluster(cl)
View(selected_stocks)
View(selected_stocks)
test_df$marketcap
model_evaluation %>%
kable(caption = "Performance metrics of tested models",
digits  = 3) %>%
kable_classic(full_width = F,
html_font  = "Times New Roman")
nn_model
# Enable parallel processing
num_cores <- detectCores() - 3
cl <- makePSOCKcluster(num_cores) # Use most cores, or specify
registerDoParallel(cl)
tunegrid_pca_nn <-  expand.grid(size  = c(5, 10, 15),
decay = c(0.001, 0.05))
# Training the NN-model
nn_model <- train(retx ~ .,
data       = train_df,
preProcess = c("center", "scale"),
trControl  = train_control,
tuneGrid   = tunegrid_pca_nn,
metric     = "MAE",
verbose = T,
allowParalell = T,
method     = "pcaNNet")
nn_predictiosn <- predict(nn_model, test_df)
postResample(nn_predictiosn, test_df$retx)
pca_nn_model <- nn_model
tunegrid_nn <-  expand.grid(size  = c(5, 7, 15),
decay = c(0.001, 0.005, 0.05))
# Training the NN-model
nn_model <- train(retx ~ .,
data       = train_df,
preProcess = c("center", "scale"),
trControl  = train_control,
tuneGrid   = tunegrid_nn,
metric     = "MAE",
verbose = T,
allowParalell = T,
method     = "nnet")
pca_nn_model
nn_model
mlp_grid<-expand.grid(layer1=10,
layer2=10,
layer3=10)
multi_hidden_layer_model <- train(retx ~ .,
data       = train_df,
preProcess = c("center", "scale"),
trControl  = train_control,
tuneGrid   = mlp_grid,
metric     = "MAE",
verbose = T,
allowParalell = T,
method     = "mlpML")
modelList <- list(knn_model, pca_nn_model, nn_model, gam_model, bayesian_ridge_model)   # List of all models
model_evaluation <- evaluate_models(modelList, train_df,  test_df)  %>%  arrange("Test MAE")
model_evaluation
model_evaluation %>%
arrange(`Test MAE`)
model_evaluation %>%
arrange(`Test MAE`) %>%
kable(caption = "Performance metrics of tested models",
digits  = 4)
model_evaluation %>%
arrange(`Test MAE`) %>%
kable(caption = "Performance metrics of tested models",
digits  = 4) %>%
kable_classic(full_width = F,
html_font  = "Times New Roman")
selected_model <- nn_model
select_stocks <- function(test_df, selected_model) {
#' @description:         Function that selects stocks based on predictability
#'                       with their performance metrics
#'
#' @param test_df        Passing a test data frame
#' @param selected_model Passing a selected model
#' @return               Companies with highest predictability
companies <- test_df$permno %>% unique()
#test_df  %<>% left_join(company_names_df, by = "permno") # merge with company names
company_predictability <- tibble()
for (company in companies) {
company_data <- test_df %>%
filter(permno == company)
company_predictions <- predict(selected_model, company_data)
company_performance_metrics <- postResample(pred = company_predictions,
obs = test_df$retx)
company_predictability %<>% bind_rows(
tibble("Company name"       = get_company_name(company_data$permno[1]),
"Company identifier" = company_data$permno[1],
"Test RMSE"          = company_performance_metrics[[1]],
"Test MAE"           = company_performance_metrics[[3]])
)
}
return (company_predictability)
}
selected_stocks <- select_stocks(test_df, selected_model)
# Printing the selected stocks with kable extra
selected_stocks %>%
arrange(desc("Test MAE")) %>%
kable(caption = "10 stocks of highest predictability",
digits  = 3)  %>%
kable_classic(full_width = F,
html_font = "Times New Roman")
selected_stocks %>%
arrange(`Test MAE`) %>%
kable(caption = "10 stocks of highest predictability",
digits  = 3)  %>%
kable_classic(full_width = F,
html_font = "Times New Roman")
View(selected_stocks)
selected_model <- bayesian_ridge_model
select_stocks <- function(test_df, selected_model) {
#' @description:         Function that selects stocks based on predictability
#'                       with their performance metrics
#'
#' @param test_df        Passing a test data frame
#' @param selected_model Passing a selected model
#' @return               Companies with highest predictability
companies <- test_df$permno %>% unique()
#test_df  %<>% left_join(company_names_df, by = "permno") # merge with company names
company_predictability <- tibble()
for (company in companies) {
company_data <- test_df %>%
filter(permno == company)
company_predictions <- predict(selected_model, company_data)
company_performance_metrics <- postResample(pred = company_predictions,
obs = test_df$retx)
company_predictability %<>% bind_rows(
tibble("Company name"       = get_company_name(company_data$permno[1]),
"Company identifier" = company_data$permno[1],
"Test RMSE"          = company_performance_metrics[[1]],
"Test MAE"           = company_performance_metrics[[3]])
)
}
return (company_predictability)
}
selected_stocks <- select_stocks(test_df, selected_model)
# Printing the selected stocks with kable extra
selected_stocks %>%
arrange(`Test MAE`) %>%
kable(caption = "10 stocks of highest predictability",
digits  = 6)  %>%
kable_classic(full_width = F,
html_font = "Times New Roman")
# Saving the models ------------------------------------------------------------
#save(knn_model, svm_model, gbm_model, file = "model_results/models.Rdata")
save(knn_model, pca_nn_model, nn_model, gam_model, bayesian_ridge_model, file = "models/models.Rdata")
mlp_grid<-expand.grid(layer1=10,
layer2=10,
layer3=10)
multi_hidden_layer_model <- train(retx ~ .,
data       = train_df,
preProcess = c("center", "scale"),
trControl  = train_control,
tuneGrid   = mlp_grid,
metric     = "MAE",
verbose = T,
allowParalell = T,
method     = "mlpML")
# Saving the models ------------------------------------------------------------
#save(knn_model, svm_model, gbm_model, file = "model_results/models.Rdata")
save(knn_model, pca_nn_model, multi_hidden_layer_model , nn_model, gam_model, bayesian_ridge_model, file = "models/models.Rdata")
evaluate_models <- function(modelList, train_df, test_df) {
#' @description     Function that evaluates the model both on the training set
#'                  and the test set by returning RMSE and MAE
#'
#' @param modelList Passing a list with fitted models
#' @param test_df   Passing test data frame
#' @return          Returns a tibble of test and validation metrics
model_performance <- tibble()
for (model in modelList) {
test_predictions          <- predict(model, newdata = test_df)
train_predictions         <- predict(model, newdata = train_df)
test_performance_metrics  <- postResample(pred = test_predictions,
obs = test_df$retx)
train_performance_metrics <- postResample(pred = train_predictions,
obs = train_df$retx)
model_performance %<>% bind_rows(
tibble(
"Model name"    =  model$method,
"Training RMSE" = train_performance_metrics[[1]],
"Training MAE"  = train_performance_metrics[[3]],
"Test RMSE"     = test_performance_metrics[[1]],
"Test MAE"      = test_performance_metrics[[3]]
)
)
}
return (model_performance)
}
modelList <- list(knn_model, multi_hidden_layer_model, nn_model, gam_model, bayesian_ridge_model)   # List of all models
model_evaluation <- evaluate_models(modelList, train_df,  test_df)  %>%  arrange("Test MAE")
model_evaluation
mlp_preds <- predict(multi_hidden_layer_model, test_df)
postResample(mlp_preds, test_df$retx)
mlp_grid_large <-expand.grid(layer1=15,
layer2=15,
layer3=15)
multi_hidden_layer_model <- train(retx ~ .,
data       = train_df,
preProcess = c("center", "scale"),
trControl  = train_control,
tuneGrid   = mlp_grid_large,
metric     = "MAE",
verbose = T,
allowParalell = T,
method     = "mlpML")
mlp_grid<-expand.grid(layer1=15,
layer2=15,
layer3=15)
mlp_weight_decay_model <- train(retx ~ .,
data       = train_df,
preProcess = c("center", "scale"),
trControl  = train_control,
tuneGrid   = mlp_grid,
metric     = "MAE",
verbose = T,
allowParalell = T,
method     = "mlpWeightDecayML")
# Stop cluster
stopCluster(cl)
