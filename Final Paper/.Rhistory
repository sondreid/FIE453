company_names_df <- read.csv(file = "data/names.csv")
feature_names_df <- read.delim(file = "data/compustat-fields.txt")
company_names_df %<>% rename_with(tolower)
feature_names_df %<>% rename_with(tolower)
merged %<>% rename_with(tolower)
# Subset of 100 companies ------------------------------------------------------
permno_top <- (merged %>%
select(permno) %>%
unique() %>%
head(1000))$permno
df <- merged %>% filter(permno %in% permno_top)
company_names_df <- read.csv(file = "data/names.csv")
# Load and read data -----------------------------------------------------------
load("data/merged.Rdata")
#company_names_df <- read.csv(file = "data/names.csv")
#feature_names_df <- read.delim(file = "data/compustat-fields.txt")
#company_names_df %<>% rename_with(tolower)
#feature_names_df %<>% rename_with(tolower)
merged %<>% rename_with(tolower)
# Subset of 100 companies ------------------------------------------------------
permno_top <- (merged %>%
select(permno) %>%
unique() %>%
head(1000))$permno
df <- merged %>% filter(permno %in% permno_top)
df$permno %>% unique() %>% length()
# Subset of 100 companies ------------------------------------------------------
permno_top <- (merged %>%
select(permno) %>%
unique() %>%
head(1000))$permno
df_reduced <- merged %>% filter(permno %in% permno_top)
# Feature selection functions --------------------------------------------------
remove_cols_only_zero_and_NA <- function(df, print_removed_cols = F) {
#'@description Function that removes columns containing only zeros and NAs
#'
#'@param df    Passing a data frame
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without columns that containing only zeros and NAs
cols <- df %>% apply(MARGIN = 2, function(x) (sum(x==0, na.rm = T) + sum(is.na(x)))/length(x))
cols <- cols[cols == 1] %>% as.data.frame() %>% rownames()
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return (df %>% select(-cols))
}
remove_NA <- function(df, ratio, print_removed_cols = F){
#'@description Function that removes columns containing NAs beyond a given
#'             ratio
#'
#'@param df    Passing a data frame
#'@param ratio Passing a upper limit NA ratio
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without columns containing NAs beyond given ratio
cols <- df %>% apply(MARGIN = 2, function(x) sum(is.na(x))/length(x))
cols <- cols[cols >= ratio] %>% as.data.frame() %>% rownames()
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% select(-cols))
}
remove_nzv <- function(df, print_removed_cols = F){
#'@description Function that removes near zero variance columns
#'
#'@param df    Passing a data frame
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without columns near zero variance columns
rec <- recipe(retx ~ .,
data = df)
cols <- (rec %>%
step_nzv(all_predictors()) %>%
prep(df) %>%
tidy(number = 1))$terms
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% select(-cols))
}
remove_hcv <- function(df, threshold = 0.9, print_removed_cols = F){
#'@description Function that removes highly correlated features
#'
#'@param df    Passing a data frame
#'@param treshold Correlation beneath this treshold
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without highly correlated features
numeric_cols <- df %>% lapply(is.numeric) %>% unlist()
rec <- recipe(retx ~ .,
data = df[numeric_cols])
cols <- (rec %>%
step_corr(all_predictors(),
threshold = threshold) %>%
prep(df[numeric_cols]) %>%
tidy(number = 1))$terms
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% select(-cols))
}
replace_NA_with_mean <- function(df, print_replaced_cols = F){
#'@description Function that replaces NA with column means
#'
#'@param df    Passing a data frame
#'@param print_replaced_cols True if user want to print replaced columns
#'@return      Data frame NA-replaced column means
na_cols <- df %>% apply(MARGIN = 2, function(x) any(is.na(x)))
numeric_cols <- df[na_cols] %>% lapply(is.numeric) %>% unlist()
col_means <- df[na_cols] %>% colMeans(na.rm = T)
col_names <- col_means %>% names()
for (col in col_names){
df[col] <- df[col][[1]] %>% replace_na(col_means[col])
}
if(print_replaced_cols) cat("Columns replaced: ", col_names, "\n\n")
return(df)
}
# Testing ----------------------------------------------------------------------
df_reduced %<>%
remove_cols_only_zero_and_NA(print_removed_cols = T) %>%
remove_NA(0.2, print_removed_cols = T) %>%
remove_nzv(print_removed_cols = T) %>%
remove_hcv(0.9, print_removed_cols = T) %>%
replace_NA_with_mean(print_replaced_cols = T)
recipe
??recipe
library(tidymodels)
install.packages("recipes")
install.packages("recipes")
library(tidymodels)
install.packages("ipred")
library(tidymodels)
install.packages("tidymodels")
install.packages("tidymodels")
install.packages("tidymodels")
library(tidymodels)
update.packges()
update.packages()
library(tidymodels)
remove.packages("tidymodels")
remove.packages("recipes")
install.packages("tidymodels")
remove.packages("ipred")
install.packages("tidymodels")
library(tidymodels)
remove.packages("ipred")
install.packages("ipred")
library(tidymodels)
library(tidymodels)
remove.packages("ipred")
install.packages("ipred")
# Libraries --------------------------------------------------------------------
library(tidyverse)
library(magrittr)
library(tidymodels)
library(randomForest)
library(caret)
library(doParallel)
# Set WD -----------------------------------------------------------------------
#setwd("~/OneDrive - Norges HandelshÃ¸yskole/MASTER/FIE453/FinalExam/FIE453/Final Paper")
# Load and read data -----------------------------------------------------------
load("data/merged.Rdata")
#company_names_df <- read.csv(file = "data/names.csv")
#feature_names_df <- read.delim(file = "data/compustat-fields.txt")
#company_names_df %<>% rename_with(tolower)
#feature_names_df %<>% rename_with(tolower)
merged %<>% rename_with(tolower)
# Subset of 100 companies ------------------------------------------------------
permno_top <- (merged %>%
select(permno) %>%
unique() %>%
head(1000))$permno
df_reduced <- merged %>% filter(permno %in% permno_top)
# Feature selection functions --------------------------------------------------
remove_cols_only_zero_and_NA <- function(df, print_removed_cols = F) {
#'@description Function that removes columns containing only zeros and NAs
#'
#'@param df    Passing a data frame
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without columns that containing only zeros and NAs
cols <- df %>% apply(MARGIN = 2, function(x) (sum(x==0, na.rm = T) + sum(is.na(x)))/length(x))
cols <- cols[cols == 1] %>% as.data.frame() %>% rownames()
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return (df %>% select(-cols))
}
remove_NA <- function(df, ratio, print_removed_cols = F){
#'@description Function that removes columns containing NAs beyond a given
#'             ratio
#'
#'@param df    Passing a data frame
#'@param ratio Passing a upper limit NA ratio
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without columns containing NAs beyond given ratio
cols <- df %>% apply(MARGIN = 2, function(x) sum(is.na(x))/length(x))
cols <- cols[cols >= ratio] %>% as.data.frame() %>% rownames()
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% select(-cols))
}
remove_nzv <- function(df, print_removed_cols = F){
#'@description Function that removes near zero variance columns
#'
#'@param df    Passing a data frame
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without columns near zero variance columns
rec <- recipe(retx ~ .,
data = df)
cols <- (rec %>%
step_nzv(all_predictors()) %>%
prep(df) %>%
tidy(number = 1))$terms
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% select(-cols))
}
remove_hcv <- function(df, threshold = 0.9, print_removed_cols = F){
#'@description Function that removes highly correlated features
#'
#'@param df    Passing a data frame
#'@param treshold Correlation beneath this treshold
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without highly correlated features
numeric_cols <- df %>% lapply(is.numeric) %>% unlist()
rec <- recipe(retx ~ .,
data = df[numeric_cols])
cols <- (rec %>%
step_corr(all_predictors(),
threshold = threshold) %>%
prep(df[numeric_cols]) %>%
tidy(number = 1))$terms
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% select(-cols))
}
replace_NA_with_mean <- function(df, print_replaced_cols = F){
#'@description Function that replaces NA with column means
#'
#'@param df    Passing a data frame
#'@param print_replaced_cols True if user want to print replaced columns
#'@return      Data frame NA-replaced column means
na_cols <- df %>% apply(MARGIN = 2, function(x) any(is.na(x)))
numeric_cols <- df[na_cols] %>% lapply(is.numeric) %>% unlist()
col_means <- df[na_cols] %>% colMeans(na.rm = T)
col_names <- col_means %>% names()
for (col in col_names){
df[col] <- df[col][[1]] %>% replace_na(col_means[col])
}
if(print_replaced_cols) cat("Columns replaced: ", col_names, "\n\n")
return(df)
}
# Testing ----------------------------------------------------------------------
df_reduced %<>%
remove_cols_only_zero_and_NA(print_removed_cols = T) %>%
remove_NA(0.2, print_removed_cols = T) %>%
remove_nzv(print_removed_cols = T) %>%
remove_hcv(0.9, print_removed_cols = T) %>%
replace_NA_with_mean(print_replaced_cols = T)
# Random Forest ----------------------------------------------------------------
perform_train_test_split <- function(df, train_ratio = 0.8) {
#' @Description: Ensures an equal amount of companies in each set
#'
#' @df:    The dataframe to be split
#' @ratio: Ratio of training data, (validation and test set to equal length)
#' @return: A list of three dataframes: training, validation and test sets
set.seed(123)
all_companies <- df$permno %>% unique()
train_indices <- sample(1:length(all_companies), floor(length(all_companies)* train_ratio))
train_companies <- all_companies[train_indices]
test_companies <- all_companies[-train_indices]
train_sample <- df %>% filter(permno %in% train_companies)
test_sample  <- df %>% filter(permno %in% test_companies)
return (
list(train_sample, test_sample)
)
}
find_company_observations <- function(df, minimum_obserations) {
#'
#' @description: Finds companies that have less than a minimum amount of observations
all_companies <- df$permno %>% unique()
df %<>% group_by(permno) %>%
summarise(count = n()) %>%
ungroup() %>%
filter(count < minimum_obserations) %>%
arrange(desc(count))
return(df)
}
low_observation_count_companies <- find_company_observations(df_reduced, 50)
df_reduced <- df_reduced %>% anti_join(low_observation_count_companies) # Cut companies with fewer than 50 observations (they cannot be reliably predicted)
# Train and test split
train_test_reduced <- perform_train_test_split(df_reduced, train_ratio = 0.8)
train_df_reduced <- train_test_reduced[[1]]
test_df_reduced <- train_test_reduced[[2]]
# Check for similar rows
test_df_reduced %>% inner_join(train_df_reduced, by = "permno") %>% nrow()
#company_names_df <- read.csv(file = "data/names.csv")
feature_names_df <- read.delim(file = "descriptions/compustat-fields.txt")
feature_names_df
df_reduced
test_df_reduced
mtry <- sqrt(ncol(x))
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
y <- train_df_reduced$retx
x <- train_df_reduced %>% select(-retx)
control <- trainControl(method = "cv",
number = 5,
verboseIter = TRUE)
set.seed(1)
mtry <- sqrt(ncol(x))
mtry <- as.numeric(sqrt(ncol(x)))
round(2.5)
mtry <- round(sqrt(ncol(x)))
tunegrid <- expand.grid(.mtry = 2)
start_time <- Sys.time()
rf <- train(x, y,
method = "rf",
importance = TRUE,
tuneGrid = tunegrid,
trControl = control)
end_time <- Sys.time()
# Most important features
varImp(rf)
tunegrid_knn <- expand.grid(k = 5:10)
knn <- train(retx~,
data = train_df_reduced,
method = "knn",
tunegrid = tunegrid_knn,
trControl = train_control,
preProcess = c("center","scale"),
allowParalell=TRUE)
knn
tunegrid_knn <- expand.grid(k = 5:10)
knn <- train(retx~.,
data = train_df_reduced,
method = "knn",
tunegrid = tunegrid_knn,
trControl = train_control,
preProcess = c("center","scale"),
allowParalell=TRUE)
train_control <- trainControl(method = "cv",
number = 5,
verboseIter = TRUE,
classProbs = TRUE,
summaryFunction = twoClassSummary)
knn <- train(retx~.,
data = train_df_reduced,
method = "knn",
tunegrid = tunegrid_knn,
trControl = train_control,
preProcess = c("center","scale"),
allowParalell=TRUE)
train_control <- trainControl(method = "cv",
number = 5,
verboseIter = TRUE,
summaryFunction = twoClassSummary)
expanded_summary  <- function(data, lev = NULL, model = NULL){
a1 <- defaultSummary(data, lev, model)
c1 <- prSummary(data, lev, model)
out <- c(a1, b1, c1)
out}
train_control <- trainControl(method = "cv",
number = 5,
verboseIter = T,
savePredictions = T,
summaryFunction = expanded_summary)
tunegrid_knn <- expand.grid(k = 5:10)
knn <- train(retx~.,
data = train_df_reduced,
method = "knn",
tunegrid = tunegrid_knn,
trControl = train_control,
preProcess = c("center","scale"),
allowParalell=TRUE)
knn
install.packages("MLmetrics")
library(Mlmetrics)
library(mlmetrics)
library(MLmetrics)
knn <- train(retx~.,
data = train_df_reduced,
method = "knn",
tunegrid = tunegrid_knn,
trControl = train_control,
preProcess = c("center","scale"),
allowParalell=TRUE)
knn <- train(retx~.,
data = train_df_reduced,
method = "knn",
tunegrid = tunegrid_knn,
preProcess = c("center","scale"),
allowParalell=TRUE)
train_control <- trainControl(method = "cv",
number = 5,
verboseIter = T,
savePredictions = T,
summaryFunction = defaultSummary )
knn <- train(retx~.,
data = train_df_reduced,
trControl  = train_control,
method = "knn",
tunegrid = tunegrid_knn,
preProcess = c("center","scale"),
allowParalell=TRUE)
train_control <- trainControl(method = "cv",
number = 5,
verboseIter = T,
savePredictions = T,
summaryFunction = defaultSummary )
knn <- train(retx~.,
data = train_df_reduced,
trControl  = train_control,
method = "knn",
tunegrid = tunegrid_knn,
preProcess = c("center","scale"),
allowParalell=TRUE)
df_reduced %<>% select(-ret)
df_reduced %<>% head(10000)
# Train and test split
train_test_reduced <- perform_train_test_split(df_reduced, train_ratio = 0.8)
train_df_reduced <- train_test_reduced[[1]]
test_df_reduced <- train_test_reduced[[2]]
# Check for similar rows
test_df_reduced %>% inner_join(train_df_reduced, by = "permno") %>% nrow()
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
expanded_summary  <- function(data, lev = NULL, model = NULL){
a1 <- defaultSummary(data, lev, model)
c1 <- prSummary(data, lev, model)
out <- c(a1, b1, c1)
out}
train_control <- trainControl(method = "cv",
number = 5,
verboseIter = T,
savePredictions = T,
summaryFunction = defaultSummary )
tunegrid_knn <- expand.grid(k = 5:10)
knn <- train(retx~.,
data = train_df_reduced,
trControl  = train_control,
method = "knn",
tunegrid = tunegrid_knn,
preProcess = c("center","scale", "pca"),
allowParalell=TRUE)
knn
knn <- train(retx~.,
data = train_df_reduced,
trControl  = train_control,
method = "knn",
tunegrid = tunegrid_knn,
preProcess = c("center","scale", "pca"),
allowParalell=TRUE)
knn
summary(knn)
tunegrid_knn <- expand.grid(k = 5:25)
knn <- train(retx~.,
data = train_df_reduced,
trControl  = train_control,
method = "knn",
tunegrid = tunegrid_knn,
preProcess = c("center","scale", "pca"),
allowParalell=TRUE)
knn
summary(knn)
tunegrid_knn <- expand.grid(k = 5:25)
knn <- train(retx~.,
data = train_df_reduced,
trControl  = train_control,
method = "knn",
metric = "MAE", # Which metric makes the most sense to use RMSE or MAE. Leaning towards MAE
tunegrid = tunegrid_knn,
preProcess = c("center","scale", "pca"),
allowParalell=TRUE)
knn
knn <- train(retx~.,
data = train_df_reduced,
trControl  = train_control,
method = "knn",
metric = "MAE", # Which metric makes the most sense to use RMSE or MAE. Leaning towards MAE
tunelength = 20,
preProcess = c("center","scale", "pca"),
allowParalell=TRUE)
knn
summary(knn)
tunegrid_svm <- expand.grid(C = seq(0, 2, length = 20)) # Try variations of margin C
svm_model_all_pca <- caret::train(retx~.,
data = train_df_reduced,
method = "svmRadial",
data = train_df,
trControl  = train_control,
preProcess = c("center", "scale", "pca"),
allowParallel=TRUE)
tunegrid_svm <- expand.grid(C = seq(0, 2, length = 20)) # Try variations of margin C
svm_model_all_pca <- caret::train(retx~.,
data = train_df_reduced,
method = "svmRadial",
trControl  = train_control,
tunegrid = tunegrid_svm,
preProcess = c("center", "scale", "pca"),
allowParallel=TRUE)
svm_model_all_pca
svm_model_all_pca$bestTune
svm_model_all_pca$results
svm_model_all_pca$results$MAE
svm_model_all_pca$results$MAE %>% min()
mtry <- round(sqrt(ncol(x)))
tunegrid_rf <- expand.grid(.mtry = 2)
start_time <- Sys.time()
rf <- train(retx~.,
data = train_df_reduced,
method = "rf",
importance = TRUE,
tuneGrid = tunegrid_rf,
trControl = train_control)
end_time <- Sys.time()
# Most important features
varImp(rf)
load("data/merged.Rdata")
#company_names_df <- read.csv(file = "data/names.csv")
feature_names_df <- read.delim(file = "descriptions/compustat-fields.txt")
#company_names_df %<>% rename_with(tolower)
#feature_names_df %<>% rename_with(tolower)
merged %<>% rename_with(tolower)
## Variables that cannot be inclduded with dependent variable RETX
exlcluded_variables <- c("ret", "prc", "vwretd") # vwretd: market excess return
merged %<>% select(-exlcluded_variables)
merged$retq
