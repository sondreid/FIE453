scale_py <- function(x) {
out <- scale(x, scale= lapply(x, 2, sd) * sqrt(N-1/N))
return(out)
}
scale_py <- function(x) {
N <- nrow(x)
out <- scale(x, scale= lapply(x, 2, sd) * sqrt(N-1/N))
return(out)
}
train_df_reduced[, "shrout"]
scale_py(train_df_reduced$shrout)
apply(list(1,2,3), 2, sd)
apply(c(1,2,3), 2, sd)
scale_py <- function(x) {
N <- nrow(x)
out <- scale(x, scale= apply(x, 2, sd) * sqrt(N-1/N))
return(out)
}
scale_py(train_df_reduced)
scale_py(train_df_reduced %>% dplyr::select(-costat))
train_df_reduced_scaled <- train_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
mutate(costat = train_df_reduced$costat)
train_df_reduced_scaled <- train_df_reduced %>% dplyr::select(-costat) %>%
scale_py()
train_df_reduced_scaled
train_df_reduced_scaled <- train_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = train_df_reduced$costat)
train_df_reduced_scaled
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced_scaled,
dropout_rates = list(0),
num_layers = 1,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
learning_rates = list(0.001, 0.1, 0.9),
patience_list = list(1, 5, 20,25),
verbose = 0
)
grid_search_nn_model_generaL_optimizer <- function(model_train_df, dropout_rates, num_layers,
learning_rates, epochs, batch_sizes, patience_list,  verbose) {
selected_spec <- feature_spec(model_train_df, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
best_MAE <- Inf
best_model <- NA
best_history <- NA
batch_normalizations <- list(T, F)
if (num_layers == 1) {batch_normalizations <- list(F)}
for (dropout_rate in dropout_rates) {
for (learn_rate in learning_rates) {
for (batch_normalization in batch_normalizations) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
selected_model = build_model(model_train_df, selected_spec, num_layers, batch_normalization, dropout_rate)
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience + 10)
optimizer = optimizer_adam(learning_rate = learn_rate) # Adam optimizer at different starting learning rates
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epochs,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[mae_list_length]]
print(paste("> MAE of new model", new_mae))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
print(paste(">Batch size", batch_size))
print(paste(">dropout_rate", dropout_rate))
print(paste("> Learning rate", learn_rate))
print(paste("> Patience", patience))
print(paste(">Batch normalization", batch_normalization))
}
}
}
}
}
}
return (list(best_model, best_history))
}
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced_scaled,
dropout_rates = list(0),
num_layers = 1,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
learning_rates = list(0.001, 0.1, 0.9),
patience_list = list(1, 5, 20,25),
verbose = 0
)
predictions_1_nn_model <- best_model_nn_1_layer_test[[1]] %>% predict(test_df_reduced %>% dplyr::select(-retx, -permno))
predictions_1_nn_model[ , 1]
postResample(predictions_1_nn_model[ , 1], test_df_reduced$retx)
best_model_nn_1_layer_test
best_model_nn_1_layer_test[[2]]
best_model_nn_1_layer_test[[2]]$params
best_model_nn_1_layer_test[[1]]$compile
best_model_nn_1_layer_test[[1]]$compile()
best_model_nn_1_layer_test[[1]] %>% compile()
best_model_nn_1_layer_test
test_df_reduced_scaled <- test_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = train_df_reduced$costat)
test_df_reduced$costat
View(test_df_reduced)
sapply(test_df_reduced, class)
test_df_reduced_scaled <- test_df_reduced %>% dplyr::select(-costat)
test_df_reduced_scaled <- test_df_reduced %>% dplyr::select(-costat) %>%
scale_py()
test_df_reduced_scaled <- test_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = test_df_reduced$costat)
predictions_1_nn_model <- best_model_nn_1_layer_test[[1]] %>% predict(test_df_reduced_scaled %>% dplyr::select(-retx, -permno))
predictions_1_nn_model[ , 1]
postResample(predictions_1_nn_model[ , 1], test_df_reduced$retx)
save_model_hdf5(best_model_nn_1_layer_test[[1]], overwrite = T, filepath = "test_model.h5")
test <- load_model_hdf5("test_model.h5")
test
grid_search_nn_model_generaL_optimizer <- function(model_train_df, dropout_rates, num_layers,
learning_rates, epochs, batch_sizes, patience_list,  verbose) {
selected_spec <- feature_spec(model_train_df, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scale_py()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
best_MAE <- Inf
best_model <- NA
best_history <- NA
batch_normalizations <- list(T, F)
if (num_layers == 1) {batch_normalizations <- list(F)}
for (dropout_rate in dropout_rates) {
for (learn_rate in learning_rates) {
for (batch_normalization in batch_normalizations) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
selected_model = build_model(model_train_df, selected_spec, num_layers, batch_normalization, dropout_rate)
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience + 10)
optimizer = optimizer_adam(learning_rate = learn_rate) # Adam optimizer at different starting learning rates
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epochs,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[mae_list_length]]
print(paste("> MAE of new model", new_mae))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
print(paste(">Batch size", batch_size))
print(paste(">dropout_rate", dropout_rate))
print(paste("> Learning rate", learn_rate))
print(paste("> Patience", patience))
print(paste(">Batch normalization", batch_normalization))
}
}
}
}
}
}
return (list(best_model, best_history))
}
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced_scaled,
dropout_rates = list(0),
num_layers = 1,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
learning_rates = list(0.001, 0.1, 0.9),
patience_list = list(1, 5, 20,25),
verbose = 0
)
grid_search_nn_model_generaL_optimizer <- function(model_train_df, dropout_rates, num_layers,
learning_rates, epochs, batch_sizes, patience_list,  verbose) {
selected_spec <- feature_spec(model_train_df, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scale()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
best_MAE <- Inf
best_model <- NA
best_history <- NA
batch_normalizations <- list(T, F)
if (num_layers == 1) {batch_normalizations <- list(F)}
for (dropout_rate in dropout_rates) {
for (learn_rate in learning_rates) {
for (batch_normalization in batch_normalizations) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
selected_model = build_model(model_train_df, selected_spec, num_layers, batch_normalization, dropout_rate)
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience + 10)
optimizer = optimizer_adam(learning_rate = learn_rate) # Adam optimizer at different starting learning rates
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epochs,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[mae_list_length]]
print(paste("> MAE of new model", new_mae))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
print(paste(">Batch size", batch_size))
print(paste(">dropout_rate", dropout_rate))
print(paste("> Learning rate", learn_rate))
print(paste("> Patience", patience))
print(paste(">Batch normalization", batch_normalization))
}
}
}
}
}
}
return (list(best_model, best_history))
}
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced_scaled,
dropout_rates = list(0),
num_layers = 1,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
learning_rates = list(0.001, 0.1, 0.9),
patience_list = list(1, 5, 20,25),
verbose = 0
)
train_df_reduced_scaled <- train_df_reduced %>% dplyr::select(-costat) %>%
scale_py()
train_df_reduced_scaled
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled %>% scale_py()
train_df_reduced_scaled <- train_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = train_df_reduced$costat)
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced_scaled,
dropout_rates = list(0),
num_layers = 1,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
learning_rates = list(0.001, 0.1, 0.9),
patience_list = list(1, 5, 20,25),
verbose = 0
)
test_df_reduced_scaled <- test_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = test_df_reduced$costat)
predictions_1_nn_model <- best_model_nn_1_layer_test[[1]] %>% predict(test_df_reduced_scaled %>% dplyr::select(-retx, -permno))
predictions_1_nn_model[ , 1]
postResample(predictions_1_nn_model[ , 1], test_df_reduced$retx)
predictions_2_nn_model <- best_model_nn_2_layer_all[[1]] %>% predict(test_df %>% dplyr::select(-retx, -permno))
predictions_2_nn_model[ , 1]
postResample(predictions_2_nn_model[ , 1], test_df$retx)
make_0_benchmark(test_df)
postResample(predictions_1_nn_model[ , 1], test_df$retx)
predictions_1_nn_model <- best_model_nn_1_layer_all[[1]] %>% predict(test_df %>% dplyr::select(-retx))
predictions_1_nn_model[ , 1]
postResample(predictions_1_nn_model[ , 1], test_df$retx)
scale_py <- function(x) {
#' R implementation which corresponds to python standard_scaler()
N <- nrow(x)
out <- scale(x, scale= lapply(x, 2, sd) * sqrt(N-1/N))
return(out)
}
grid_search_nn_model_generaL_optimizer <- function(model_train_df, dropout_rates, num_layers,
learning_rates, epochs, batch_sizes, patience_list,  verbose) {
selected_spec <- feature_spec(model_train_df, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
best_MAE <- Inf
best_model <- NA
best_history <- NA
batch_normalizations <- list(T, F)
if (num_layers == 1) {batch_normalizations <- list(F)}
for (dropout_rate in dropout_rates) {
for (learn_rate in learning_rates) {
for (batch_normalization in batch_normalizations) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
selected_model = build_model(model_train_df, selected_spec, num_layers, batch_normalization, dropout_rate)
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience + 10)
optimizer = optimizer_adam(learning_rate = learn_rate) # Adam optimizer at different starting learning rates
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epochs,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[mae_list_length]]
print(paste("> MAE of new model", new_mae))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
print(paste(">Batch size", batch_size))
print(paste(">dropout_rate", dropout_rate))
print(paste("> Learning rate", learn_rate))
print(paste("> Patience", patience))
print(paste(">Batch normalization", batch_normalization))
}
}
}
}
}
}
return (list(best_model, best_history))
}
est_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced,
dropout_rates = list(0),
num_layers = 1,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
learning_rates = list(0.005, 0.1, 0.4),
patience_list = list(1, 5, 10, 20),
verbose = 0
)
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced,
dropout_rates = list(0),
num_layers = 1,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
learning_rates = list(0.005, 0.1, 0.4),
patience_list = list(1, 5, 10, 20),
verbose = 0
)
best_model_nn_2_layers_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced,
dropout_rates = list(0, 0.3, 0.4),
num_layers = 2,
batch_sizes = list(300, 500, 1000, 7000),
epochs = 200,
optimizer = adam_opt,
patience_list = list(1, 2, 5, 20,25),
verbose = 0
)
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced_scaled,
dropout_rates = list(0),
num_layers = 1,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
learning_rates = list(0.005, 0.1, 0.4),
patience_list = list(1, 5, 10, 20),
verbose = 0
)
train_df_reduced_scaled <- train_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = train_df_reduced$costat)
test_df_reduced_scaled <- test_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = test_df_reduced$costat)
train_df_reduced_scaled <- train_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = train_df_reduced$costat)
train_df_reduced_scaled <- train_df_reduced %>% dplyr::select(-costat) %>%
scale_py()
train_df_reduced
scale_py <- function(x) {
#' R implementation which corresponds to python standard_scaler()
N <- nrow(x)
out <- scale(x, scale= apply(x, 2, sd) * sqrt(N-1/N))
return(out)
}
train_df_reduced_scaled <- train_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = train_df_reduced$costat)
scale_py <- function(x) {
#' R implementation which corresponds to python standard_scaler()
n <- nrow(x)
scaled_df <- scale(x, scale= apply(x, 2, sd) * sqrt(n-1/n))
return(scaled_df)
}
train_df_reduced_scaled <- train_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = train_df_reduced$costat)
test_df_reduced_scaled <- test_df_reduced %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = test_df_reduced$costat)
## Single layer
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced_scaled,
dropout_rates = list(0),
num_layers = 1,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
learning_rates = list(0.005, 0.1, 0.4),
patience_list = list(1, 5, 10, 20),
verbose = 0
)
best_model_nn_2_layers_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced_scaled,
dropout_rates = list(0, 0.3, 0.4),
num_layers = 2,
batch_sizes = list(300, 500, 1000, 7000),
epochs = 200,
optimizer = adam_opt,
patience_list = list(1, 2, 5, 20,25),
verbose = 0
)
best_model_nn_3_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced_scaled,
dropout_rates = list(0,0.3, 0.4),
num_layers = 3,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
learning_rates = list(0.005, 0.1, 0.4),
patience_list = list(1, 5, 10, 20),
verbose = 0
)
predictions_3_nn_model <- best_model_nn_3_layer_test[[1]] %>% predict(test_df_reduced_scaled %>% dplyr::select(-retx, -permno))
predictions_3_nn_model[ , 1]
postResample(predictions_3_nn_model[ , 1], test_df_reduced_scaled$retx)
postResample(predictions_3_nn_model[ , 1], test_df$retx)
postResample(predictions_3_nn_model[ , 1], test_df_reduced$retx)
predictions_2_nn_model <- best_model_nn_2_layers_test[[1]] %>% predict(test_df_reduced_scaled %>% dplyr::select(-retx, -permno))
predictions_2_nn_model[ , 1]
postResample(predictions_2_nn_model[ , 1], test_df_reduced$retx)
postResample(predictions_1_nn_model[ , 1], test_df_reduced$retx)
predictions_1_nn_model <- best_model_nn_1_layer_test[[1]] %>% predict(test_df_reduced_scaled %>% dplyr::select(-retx, -permno))
predictions_1_nn_model[ , 1]
postResample(predictions_1_nn_model[ , 1], test_df_reduced$retx)
postResample(predictions_3_nn_model[ , 1], test_df_reduced$retx)
postResample(predictions_2_nn_model[ , 1], test_df_reduced$retx)
predictions_2_nn_model[ , 1]
predictions_2_nn_model <- best_model_nn_2_layers_test[[1]] %>% predict(test_df_reduced_scaled %>% dplyr::select(-retx, -permno))
predictions_2_nn_model[ , 1]
postResample(predictions_2_nn_model[ , 1], test_df_reduced$retx)
save_model_hdf5(best_model_nn_1_layer_test[[1]], overwrite = T, filepath = "test_model.h5")
test <- load_model_hdf5("test_model.h5")
best_model_nn_1_layer_all  <- grid_search_nn_model_generaL_optimizer(train_df,
dropout_rates = list(0),
num_layers = 1,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
learning_rates = list(0.005, 0.1, 0.4),
patience_list = list(1, 5, 10, 20),
verbose = 0
)
train_df_scaled <- train_df %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = train_df_reduced$costat)
train_df_scaled <- train_df %>% dplyr::select(-costat) %>%
scale_py() %>%
as_tibble() %>%
mutate(costat = train_df$costat)
