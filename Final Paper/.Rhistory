data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 300)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 1000)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 150)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 200)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 150)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
accuracy.boosted
set.seed(8712)
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 15)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
accuracy.boosted
set.seed(8712)
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 150)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
accuracy.boosted
set.seed(8712)
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 200)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
accuracy.boosted
set.seed(8712)
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 250)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
accuracy.boosted
set.seed(8712)
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 250)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
accuracy.boosted
false.negative <- conf.table.boosted[1,2]/sum(conf.table.boosted[1,])
false.negative
false.positive <- conf.table.boosted[2,1]/sum(conf.boosted[2,])
set.seed(8712)
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 250)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
accuracy.boosted
false.negative <- conf.table.boosted[1,2]/sum(conf.table.boosted[1,])
false.negative
false.positive <- conf.table.boosted[2,1]/sum(conf.table.boosted[2,])
false.positive
set.seed(8712)
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 200)
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
accuracy.boosted
false.negative <- conf.table.boosted[1,2]/sum(conf.table.boosted[1,])
false.negative
false.positive <- conf.table.boosted[2,1]/sum(conf.table.boosted[2,])
false.positive
View(Mode)
rm(Mode)
table.clm.vs.veh_value <-
dataCar %>%
dplyr::select(clm, veh_value) %>%
group_by(clm) %>%
summarize(mean = mean(veh_value),
median = median(veh_value),
std.dev = sd(veh_value))
table.clm.vs.exposure <-
dataCar %>%
dplyr::select(clm, exposure) %>%
group_by(clm) %>%
summarize(mean = mean(exposure),
median = median(exposure),
std.dev = sd(exposure))
table.clm.vs.veh_age <-
dataCar %>%
dplyr::select(clm, veh_age) %>%
group_by(clm) %>%
summarize(mean = mean(veh_age),
median = median(veh_age),
std.dev = sd(veh_age),
mode = Mode(ven_age))
install.packages(DescTools)
install.packages("DescTools")
library(DescTools)
table.clm.vs.veh_value <-
dataCar %>%
dplyr::select(clm, veh_value) %>%
group_by(clm) %>%
summarize(mean = mean(veh_value),
median = median(veh_value),
std.dev = sd(veh_value))
table.clm.vs.exposure <-
dataCar %>%
dplyr::select(clm, exposure) %>%
group_by(clm) %>%
summarize(mean = mean(exposure),
median = median(exposure),
std.dev = sd(exposure))
table.clm.vs.veh_age <-
dataCar %>%
dplyr::select(clm, veh_age) %>%
group_by(clm) %>%
summarize(mean = mean(veh_age),
median = median(veh_age),
std.dev = sd(veh_age),
mode = Mode(ven_age))
table.clm.vs.veh_value <-
dataCar %>%
dplyr::select(clm, veh_value) %>%
group_by(clm) %>%
summarize(mean = mean(veh_value),
median = median(veh_value),
std.dev = sd(veh_value))
table.clm.vs.exposure <-
dataCar %>%
dplyr::select(clm, exposure) %>%
group_by(clm) %>%
summarize(mean = mean(exposure),
median = median(exposure),
std.dev = sd(exposure))
table.clm.vs.veh_age <-
dataCar %>%
dplyr::select(clm, veh_age) %>%
group_by(clm) %>%
summarize(mean = mean(veh_age),
median = median(veh_age),
std.dev = sd(veh_age),
mode = descTools::Mode(ven_age))
table.clm.vs.veh_value <-
dataCar %>%
dplyr::select(clm, veh_value) %>%
group_by(clm) %>%
summarize(mean = mean(veh_value),
median = median(veh_value),
std.dev = sd(veh_value))
table.clm.vs.exposure <-
dataCar %>%
dplyr::select(clm, exposure) %>%
group_by(clm) %>%
summarize(mean = mean(exposure),
median = median(exposure),
std.dev = sd(exposure))
table.clm.vs.veh_age <-
dataCar %>%
dplyr::select(clm, veh_age) %>%
group_by(clm) %>%
summarize(mean = mean(veh_age),
median = median(veh_age),
std.dev = sd(veh_age),
mode = DescTools::Mode(ven_age))
dataCar
table.clm.vs.veh_value <-
dataCar %>%
dplyr::select(clm, veh_value) %>%
group_by(clm) %>%
summarize(mean = mean(veh_value),
median = median(veh_value),
std.dev = sd(veh_value))
table.clm.vs.exposure <-
dataCar %>%
dplyr::select(clm, exposure) %>%
group_by(clm) %>%
summarize(mean = mean(exposure),
median = median(exposure),
std.dev = sd(exposure))
table.clm.vs.veh_age <-
dataCar %>%
dplyr::select(clm, veh_age) %>%
group_by(clm) %>%
summarize(mean = mean(veh_age),
median = median(veh_age),
std.dev = sd(veh_age),
mode = Mode(veh_age))
# Claim vs. vehicle value
table.clm.vs.veh_value
# Claim vs. exposure
table.clm.vs.exposure
# Claim vs. vehicle age
table.clm.vs.veh_age
table.clm.vs.veh_value <-
dataCar %>%
dplyr::select(clm, veh_value) %>%
group_by(clm) %>%
summarize(mean = mean(veh_value),
median = median(veh_value),
std.dev = sd(veh_value),
mode = Mode(veh_value))
table.clm.vs.exposure <-
dataCar %>%
dplyr::select(clm, exposure) %>%
group_by(clm) %>%
summarize(mean = mean(exposure),
median = median(exposure),
std.dev = sd(exposure),
mode = Mode(exposure))
table.clm.vs.veh_age <-
dataCar %>%
dplyr::select(clm, veh_age) %>%
group_by(clm) %>%
summarize(mean = mean(veh_age),
median = median(veh_age),
std.dev = sd(veh_age),
mode = Mode(veh_age))
# Claim vs. vehicle value
table.clm.vs.veh_value
# Claim vs. exposure
table.clm.vs.exposure
# Claim vs. vehicle age
table.clm.vs.veh_age
# Seeding at 8712
set.seed(8712)
# Using gbm to create a boosted tree model, using bernoulli distribution for categorical prediction
# Testing with some shrinkage value, landed on 200 in order to get some sensible outcome.
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 200)
# Predicting the
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000,
type = "response") > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
accuracy.boosted
false.negative <- conf.table.boosted[1,2]/sum(conf.table.boosted[1,])
false.negative
false.positive <- conf.table.boosted[2,1]/sum(conf.table.boosted[2,])
false.positive
# Seeding at 8712
set.seed(8712)
# Using gbm to create a boosted tree model, using bernoulli distribution for categorical prediction
# Testing with some shrinkage value, landed on 200 in order to get some sensible outcome.
boosted <- gbm(clm ~ exposure + veh_body + veh_age + agecat,
data = train,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 200)
# Predicting the
pred.boosted <- predict(boosted,
newdata = test,
n.trees = 1000) > 0.02
conf.table.boosted <- table(Actual = test$clm,
Predicted = pred.boosted)
conf.table.boosted
accuracy.boosted <- sum(diag(conf.table.boosted))/sum(conf.table)
accuracy.boosted
false.negative <- conf.table.boosted[1,2]/sum(conf.table.boosted[1,])
false.negative
false.positive <- conf.table.boosted[2,1]/sum(conf.table.boosted[2,])
false.positive
# Random Forest ----------------------------------------------------------------
library(randomForest)
library(caret)
?trainControl()
################################################################################
########################### Pre-processing #####################################
################################################################################
# Libraries --------------------------------------------------------------------
library(tidyverse)
library(magrittr)
library(tidymodels)
# Set WD -----------------------------------------------------------------------
setwd("~/OneDrive - Norges Handelshøyskole/MASTER/FIE453/FinalExam/FIE453/Final Paper")
# Load and read data -----------------------------------------------------------
load("data/merged.Rdata")
company_names_df <- read.csv(file = "data/names.csv")
feature_names_df <- read.delim(file = "data/compustat-fields.txt")
company_names_df %<>% rename_with(tolower)
feature_names_df %<>% rename_with(tolower)
merged %<>% rename_with(tolower)
# Subset of 100 companies ------------------------------------------------------
permno_top <- (merged %>%
select(permno) %>%
unique() %>%
head(1000))$permno
df <- merged %>% filter(permno %in% permno_top)
# Feature selection functions --------------------------------------------------
remove_cols_only_zero_and_NA <- function(df, print_removed_cols = F) {
#'@description Function that removes columns containing only zeros and NAs
#'
#'@param df    Passing a data frame
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without columns that containing only zeros and NAs
cols <- df %>% apply(MARGIN = 2, function(x) (sum(x==0, na.rm = T) + sum(is.na(x)))/length(x))
cols <- cols[cols == 1] %>% as.data.frame() %>% rownames()
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return (df %>% select(-cols))
}
remove_NA <- function(df, ratio, print_removed_cols = F){
#'@description Function that removes columns containing NAs beyond a given
#'             ratio
#'
#'@param df    Passing a data frame
#'@param ratio Passing a upper limit NA ratio
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without columns containing NAs beyond given ratio
cols <- df %>% apply(MARGIN = 2, function(x) sum(is.na(x))/length(x))
cols <- cols[cols >= ratio] %>% as.data.frame() %>% rownames()
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% select(-cols))
}
remove_nzv <- function(df, print_removed_cols = F){
#'@description Function that removes near zero variance columns
#'
#'@param df    Passing a data frame
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without columns near zero variance columns
rec <- recipe(retx ~ .,
data = df)
cols <- (rec %>%
step_nzv(all_predictors()) %>%
prep(df) %>%
tidy(number = 1))$terms
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% select(-cols))
}
remove_hcv <- function(df, threshold = 0.9, print_removed_cols = F){
#'@description Function that removes highly correlated features
#'
#'@param df    Passing a data frame
#'@param treshold Correlation beneath this treshold
#'@param print_removed_cols True if user want to print removed columns
#'@return      Data frame without highly correlated features
numeric_cols <- df %>% lapply(is.numeric) %>% unlist()
rec <- recipe(retx ~ .,
data = df[numeric_cols])
cols <- (rec %>%
step_corr(all_predictors(),
threshold = threshold) %>%
prep(df[numeric_cols]) %>%
tidy(number = 1))$terms
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% select(-cols))
}
replace_NA_with_mean <- function(df, print_replaced_cols = F){
#'@description Function that replaces NA with column means
#'
#'@param df    Passing a data frame
#'@param print_replaced_cols True if user want to print replaced columns
#'@return      Data frame NA-replaced column means
na_cols <- df %>% apply(MARGIN = 2, function(x) any(is.na(x)))
numeric_cols <- df[na_cols] %>% lapply(is.numeric) %>% unlist()
col_means <- df[na_cols] %>% colMeans(na.rm = T)
col_names <- col_means %>% names()
for (col in col_names){
df[col] <- df[col][[1]] %>% replace_na(col_means[col])
}
if(print_replaced_cols) cat("Columns replaced: ", col_names, "\n\n")
return(df)
}
# Testing ----------------------------------------------------------------------
df %<>%
remove_cols_only_zero_and_NA(print_removed_cols = T) %>%
remove_NA(0.2, print_removed_cols = T) %>%
remove_nzv(print_removed_cols = T) %>%
remove_hcv(0.9, print_removed_cols = T) %>%
replace_NA_with_mean(print_replaced_cols = T)
# Random Forest ----------------------------------------------------------------
library(randomForest)
library(caret)
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
y <- df$retx
x <- df %>% select(-retx)
control <- trainControl(method = "cv",
number = 5,
verboseIter = TRUE)
set.seed(1)
mtry <- sqrt(ncol(x))
tunegrid <- expand.grid(.mtry = mtry)
start_time <- Sys.time()
rf <- train(x, y,
method = "rf",
importance = TRUE,
tuneGrid = tunegrid,
trControl = control)
end_time <- Sys.time()
stopCluster(cl)
stopCluster(cl)
# Most important features
varImp(rf)
