step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
grid_search_nn_model <- function(model, model_train_df, model_test_df, learning_rates, momentums) {
best_MAE <- Inf
best_model <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
new_model <- model %>%
compile(
loss = "mse",
optimizer = optimizer_sgd(
learning_rate =lr,
momentum = momentum,
decay = 0.01),
metrics = list("mean_absolute_error"))
new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = 50,
batch_size = 150,
validation_split = 0.2,
verbose = 0,
callbacks = list(print_dot_callback)
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_model <- new_model
best_MAE <- mae
}
}
}
return (best_model)
}
best_model <- grid_search_nn_model(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced, learning_rates = list(0.001, 0.000005), momentums = list(0, 0.001))
predictions_5_nn_model <- best_model %>% predict(test_df_reduced %>% dplyr::select(-retx))
predictions_5_nn_model[ , 1]
postResample(predictions_5_nn_model[ , 1], test_df_reduced$retx)
make_0_benchmark(test_df_reduced)
make_0_benchmark <- function(selected_test_df) {
#' Makes a zero benchmark to compare models
#' @
benchmark_0 <- postResample(rep(0, nrow(selected_test_df)), selected_test_df$retx)
return (benchmark_0)
}
make_0_benchmark(test_df_reduced)
grid_search_nn_model <- function(model, model_train_df, model_test_df, learning_rates, momentums) {
best_MAE <- Inf
best_model <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
new_model <- model %>%
compile(
loss = "mae",
optimizer = optimizer_sgd(
learning_rate =lr,
momentum = momentum),
metrics = list("mean_absolute_error"))
new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = 50,
batch_size = 150,
validation_split = 0.2,
verbose = 1,
callbacks = list(print_dot_callback)
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_model <- new_model
best_MAE <- mae
}
}
}
return (best_model)
}
best_model <- grid_search_nn_model(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced, learning_rates = list(0.001, 0.000005), momentums = list(0, 0.001))
grid_search_nn_model <- function(model, model_train_df, model_test_df, learning_rates, momentums) {
best_MAE <- Inf
best_model <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
for (epoch in epochs) {
for (batch_size in batch_sizes) {
new_model <- model %>%
compile(
loss = "mse",
optimizer = optimizer_sgd(
learning_rate =lr,
momentum = momentum),
metrics = list("mean_absolute_error"))
new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback)
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_model <- new_model
best_MAE <- mae
}
}
}
}
}
return (best_model)
}
best_model <- grid_search_nn_model(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
learning_rates = list(0.001, 0.000005),
momentums = list(0, 0.001),
batch_sizes = list(100, 500),
epochs = list(30, 100),
verbose = 0
)
grid_search_nn_model <- function(model, model_train_df, model_test_df, learning_rates, momentums,
epochs, batch_sizes, verbose) {
best_MAE <- Inf
best_model <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
for (epoch in epochs) {
for (batch_size in batch_sizes) {
new_model <- model %>%
compile(
loss = "mse",
optimizer = optimizer_sgd(
learning_rate =lr,
momentum = momentum),
metrics = list("mean_absolute_error"))
new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback)
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_model <- new_model
best_MAE <- mae
}
}
}
}
}
return (best_model)
}
best_model <- grid_search_nn_model(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
learning_rates = list(0.001, 0.000005),
momentums = list(0, 0.001),
batch_sizes = list(100, 500),
epochs = list(30, 100),
verbose = 0
)
# Better than 0 benchmark?
make_0_benchmark(test_df_reduced)
make_0_benchmark(test_df_reduced)[[3]]
predictions_5_nn_model <- best_model %>% predict(test_df_reduced %>% dplyr::select(-retx))
predictions_5_nn_model[ , 1]
postResample(predictions_5_nn_model[ , 1], test_df_reduced$retx)
# Better than 0 benchmark?
make_0_benchmark(test_df_reduced)
make_0_benchmark(test_df_reduced)[[3]] > postResample(predictions_5_nn_model[ , 1], test_df_reduced$retx)[[3]]
predictions_5_nn_model[ , 1]
parts <- createRandomDataPartition(Boston$medv, times = 1, p = 0.2)
# Train Control
train_control <- trainControl(method = "cv", #Method does not matter
index = parts,
number = 10,
verboseIter = T,
savePredictions = T,
summaryFunction = defaultSummary)
createRandomDataPartition
??createRandomDataPartition
library(tidyverse)
library(magrittr)
library(tidymodels)
library(randomForest)
library(caret)
library(doParallel)
library(MLmetrics)
library(gbm)
library(PerformanceAnalytics)
library(kableExtra)
library(knitr)
library(monomvn)
library(kableExtra)
library(lubridate)
library(kknn)
library(nnet)
parts <- createRandomDataPartition(train_df_reduced$retx, times = 1, p = 0.2)
createDataPartition
parts <- createDataPartition(train_df_reduced$retx, times = 1, p = 0.2)
parts
parts <- createDataPartition(train_df_reduced$retx, times = 1, p = 0.2)
# Train Control
train_control <- trainControl(method = "cv", #Method does not matter
index = parts,
number = 10,
verboseIter = T,
savePredictions = T,
summaryFunction = defaultSummary)
knn_model <- train(retx ~ .,
data          = train_df_reduced,
trControl     = train_control,
method        = "knn",
metric        = "MAE",                                       # Which metric makes the most sense to use RMSE or MAE. Leaning towards MAE
tuneLength      = 10,
preProcess    = c("center", "scale"),
allowParalell = TRUE)
# Finding the KNN-model that minimizes MAE
knn_model$results$MAE %>% min() # Validation accuracy
set_random_seed
help(set_random_seed)
grid_search_nn_model <- function(model, model_train_df, model_test_df, learning_rates, momentums,
epochs, batch_sizes, verbose) {
best_MAE <- Inf
best_model <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
for (epoch in epochs) {
for (batch_size in batch_sizes) {
new_model <- model %>%
compile(
loss = "mse",
optimizer = optimizer_sgd(
learning_rate =lr,
momentum = momentum),
metrics = list("mean_absolute_error"))
new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, early_stop) #Print simplified dosts, and stop learning when validation improvements stalls
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_model <- new_model
best_MAE <- mae
}
}
}
}
}
return (best_model)
}
grid_search_nn_model_rmsprop <- function(model, model_train_df, model_test_df, learning_rates, momentums,
epochs, batch_sizes, verbose) {
best_MAE <- Inf
best_model <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
for (epoch in epochs) {
for (batch_size in batch_sizes) {
new_model <- model %>%
compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error"))
new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, early_stop) #Print simplified dosts, and stop learning when validation improvements stalls
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_model <- new_model
best_MAE <- mae
}
}
}
}
}
return (best_model)
}
best_model_rms_prop  <- grid_search_nn_model_rmsprop(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
learning_rates = list(0.01),
momentums = list(0),
batch_sizes = list(100, 500),
epochs = list(30, 100),
verbose = 0
)
best_model <- grid_search_nn_model(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
learning_rates = list(0.01, 0.0001),
momentums = list(0, 0.001),
batch_sizes = list(100, 500),
epochs = list(30, 100),
verbose = 0
)
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
grid_search_nn_model <- function(model, model_train_df, model_test_df, learning_rates, momentums,
epochs, batch_sizes, verbose) {
best_MAE <- Inf
best_model <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
for (epoch in epochs) {
for (batch_size in batch_sizes) {
new_model <- model %>%
compile(
loss = "mse",
optimizer = optimizer_sgd(
learning_rate =lr,
momentum = momentum),
metrics = list("mean_absolute_error"))
new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, early_stop) #Print simplified dosts, and stop learning when validation improvements stalls
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_model <- new_model
best_MAE <- mae
}
}
}
}
}
return (best_model)
}
grid_search_nn_model_rmsprop <- function(model, model_train_df, model_test_df, learning_rates, momentums,
epochs, batch_sizes, verbose) {
best_MAE <- Inf
best_model <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
for (epoch in epochs) {
for (batch_size in batch_sizes) {
new_model <- model %>%
compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error"))
new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, early_stop) #Print simplified dosts, and stop learning when validation improvements stalls
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_model <- new_model
best_MAE <- mae
}
}
}
}
}
return (best_model)
}
best_model_rms_prop  <- grid_search_nn_model_rmsprop(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
learning_rates = list(0.01),
momentums = list(0),
batch_sizes = list(100, 500),
epochs = list(30, 100),
verbose = 0
)
best_model <- grid_search_nn_model(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
learning_rates = list(0.01, 0.0001),
momentums = list(0, 0.001),
batch_sizes = list(100, 500),
epochs = list(30, 100),
verbose = 0
)
predictions_5_nn_model <- best_model %>% predict(test_df_reduced %>% dplyr::select(-retx))
predictions_5_nn_model[ , 1]
postResample(predictions_5_nn_model[ , 1], test_df_reduced$retx)
# Better than 0 benchmark?
make_0_benchmark(test_df_reduced)
make_0_benchmark(test_df_reduced)[[3]] > postResample(predictions_5_nn_model[ , 1], test_df_reduced$retx)[[3]]
predictions_5_nn_model <- best_model_rms_prop %>% predict(test_df_reduced %>% dplyr::select(-retx))
predictions_5_nn_model[ , 1]
postResample(predictions_5_nn_model[ , 1], test_df_reduced$retx)
best_model_rms_prop  <- grid_search_nn_model_rmsprop(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
learning_rates = list(0.01),
momentums = list(0),
batch_sizes = list(50, 80, 100, 200, 500),
epochs = list(20, 50, 70, 300),
verbose = 0
)
grid_search_nn_model_rmsprop <- function(model, model_train_df, model_test_df, learning_rates, momentums,
epochs, batch_sizes, verbose) {
best_MAE <- Inf
best_model <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
for (epoch in epochs) {
for (batch_size in batch_sizes) {
new_model <- model %>%
compile(
loss = "mse",
optimizer = optimizer_rmsprop(learning_rate = lr),
metrics = list("mean_absolute_error"))
new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, early_stop) #Print simplified dosts, and stop learning when validation improvements stalls
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_model <- new_model
best_MAE <- mae
}
}
}
}
}
return (best_model)
}
best_model_rms_prop  <- grid_search_nn_model_rmsprop(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
learning_rates = list(0.001, 0.01),
momentums = list(0),
batch_sizes = list(50, 80, 400, 1000, 10000),
epochs = list(20, 50, 70, 300),
verbose = 0
)
best_model_rms_prop  <- grid_search_nn_model_rmsprop(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
learning_rates = list(0.001, 0.01),
momentums = list(0),
batch_sizes = list(50, 80, 400, 1000, 10000),
epochs = list(20, 50, 70, 300),
verbose = 0
)
best_model_rms_prop  <- grid_search_nn_model_rmsprop(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
learning_rates = list(0.001, 0.01),
momentums = list(0),
batch_sizes = list(50, 80, 400, 1000, 10000),
epochs = list(20, 50, 70, 300),
verbose = 0
)
predictions_5_nn_model_rms_prop <- best_model_rms_prop %>% predict(test_df_reduced %>% dplyr::select(-retx))
predictions_5_nn_model_rms_prop[ , 1]
View(train_df)
postResample(predictions_5_nn_model[ , 1], test_df_reduced$retx)
# Better than 0 benchmark?
make_0_benchmark(test_df_reduced)
plot(best_model_rms_prop)
best_model_rms_prop
grid_search_nn_model_rmsprop <- function(model, model_train_df, model_test_df, learning_rates, momentums,
epochs, batch_sizes, verbose) {
best_MAE <- Inf
best_model <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
for (epoch in epochs) {
for (batch_size in batch_sizes) {
new_model <- model %>%
compile(
loss = "mse",
optimizer = optimizer_rmsprop(learning_rate = lr),
metrics = list("mean_absolute_error"))
history <- new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, early_stop) #Print simplified dosts, and stop learning when validation improvements stalls
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_model <- history
best_MAE <- mae
}
}
}
}
}
return (best_model)
}
best_model_rms_prop  <- grid_search_nn_model_rmsprop(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
learning_rates = list(0.001, 0.01),
momentums = list(0),
batch_sizes = list(50, 80, 400, 1000, 10000),
epochs = list(20, 50, 70, 300),
verbose = 0
)
