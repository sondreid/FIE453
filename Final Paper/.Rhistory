optimizer = adam_opt,
patience_list = list(20,25, 40, 50),
verbose = 1
)
best_model_nn_3_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_3_layers_reduced, train_df_reduced,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(20,25, 40, 50),
verbose = 1
)
build_nn_model_3_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dropout(0.3) %>%
#layer_batch_normalization() %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dropout(0.3) %>%
#layer_batch_normalization() %>%
layer_dense(units = 50)
model <- keras_model(input, output)
return(model)
}
nn_model_3_layers_reduced <- build_nn_model_3_layers(train_df_reduced, spec_reduced)
nn_model_3_layers <- build_nn_model_3_layers(train_df, spec)
best_model_nn_3_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_3_layers_reduced, train_df_reduced,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(20,25, 40, 50),
verbose = 1
)
sgd_opt = optimizer_sgd(learning_rate = 0.8)
best_model_nn_3_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_3_layers_reduced, train_df_reduced,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = sgd_opt,
patience_list = list(20,25, 40, 50),
verbose = 1
)
best_model_nn_3_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_3_layers_reduced, train_df_reduced,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = sgd_opt,
patience_list = list(20,25, 40, 50),
verbose = 0
)
predictions_3_nn_model <- best_model_nn_3_layer_test[[1]] %>% predict(test_df_reduced %>% dplyr::select(-retx))
predictions_3_nn_model[ , 1]
postResample(predictions_3_nn_model[ , 1], test_df_reduced$retx)
# Better than 0 benchmark?
make_0_benchmark(test_df_reduced)
best_model_nn_3_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_3_layers_reduced, train_df_reduced,
batch_sizes = list(500, 1000, 7000),
epochs = list(200),
optimizer = sgd_opt,
patience_list = list(5, 20,25, 40, 50),
verbose = 0
)
predictions_3_nn_model <- best_model_nn_3_layer_test[[1]] %>% predict(test_df_reduced %>% dplyr::select(-retx))
predictions_3_nn_model[ , 1]
postResample(predictions_3_nn_model[ , 1], test_df_reduced$retx)
best_model_nn_5_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_5_layers_reduced, train_df_reduced, test_df_reduced,
batch_sizes = list(500, 1000, 7000),
epochs = list(200),
optimizer = sgd_opt,
patience_list = list(5, 20,25, 40, 50),
verbose = 0
)
best_model_nn_5_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_5_layers_reduced, train_df_reduced,
batch_sizes = list(500, 1000, 7000),
epochs = list(200),
optimizer = sgd_opt,
patience_list = list(5, 20,25, 40, 50),
verbose = 0
)
# Train Control
parts <- createDataPartition(train_df$retx, times = 1, p = 0.2) # 20 % of training data is used for validation (i.e, hyperparameter selection)
train_control <- trainControl(method = "cv", #Method does not matter as parts dictate 20 % validation of training set
index = parts,
savePredictions = T)
# Enable parallel processing
num_cores <- detectCores() - 3
cl <- makePSOCKcluster(num_cores) # Use most cores, or specify
registerDoParallel(cl)
gbm_model <- caret::train(retx ~ .,
data       = train_df_reduced %>% mutate(costat = as.factor(costat)),
method     = "gbm",
preProcess = c("center","scale"),
metric     = "MAE",
tuneLength   = 10,
trControl  = train_control,
allowParalell = TRUE)
# Training the GBM-model
gbm_model <- caret::train(retx ~ .,
data       = train_df,
method     = "gbm",
preProcess = c("center","scale"),
metric     = "MAE",
tuneLength   = 10,
trControl  = train_control,
allowParalell = TRUE)
# Training the GBM-model
gbm_model <- caret::train(retx ~ .,
data       = train_df,
method     = "gbm",
preProcess = c("center","scale"),
metric     = "MAE",
tuneLength   = 10,
trControl  = train_control,
allowParalell = TRUE)
# Training the GBM-model
gbm_model <- caret::train(retx ~ .,
data       = train_df,
method     = "gbm",
preProcess = c("center","scale"),
metric     = "MAE",
tuneLength   = 10,
trControl  = train_control)
gbm_preds <- predict(gbm_model, test_df)
postResample(gbm_preds, test_df$retx)
gam_model <- train(retx ~ .,
data       = train_df,
preProcess = c("center", "scale"),
trControl  = train_control,
tuneLength   = 10,
metric     = "MAE",
method     = "gam")
# Training the GAM-model
gam_model <- caret::train(retx ~ .,
data       = train_df,
preProcess = c("center", "scale"),
trControl  = train_control,
tuneLength   = 10,
metric     = "MAE",
method     = "gam")
# In order to save run time one can choose to load the model results
load(file = "models/models.Rdata")
# Saving the models ------------------------------------------------------------
save(knn_model, bayesian_ridge_model, gam_model,gbm_model, file = "models/models.Rdata")
gam_preds <- predict(gam_model, test_df)
postResample(gam_preds, test_df$retx)
build_nn_model_3_layers <- function(selected_train_df, selected_spec, batch_normalization, dropout_rate) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dropout(dropout_rate)
if (batch_normalization) output %<>% layer_batch_normalization()
output  %<>% layer_dense(units = 100, activation = "relu") %>%
layer_dropout(dropout_rate)
if (batch_normalization) output %<>% layer_batch_normalization()
output %<>% layer_dense(units = 50)
model <- keras_model(input, output)
return(model)
}
build_nn_model_2_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_batch_normalization() %>%
layer_dense(units = 16)
model <- keras_model(input, output)
return(model)
}
build_nn_model_1_layer <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32)
model <- keras_model(input, output)
return(model)
}
build_model <- function(selected_train_df, selected_spec, num_layers, batch_normalization, dropout_rate) {
if (num_layers == 1) {
output_model = build_nn_model_1_layer(selected_train_df, selected_spec, batch_normalization, dropout_rate)
}
if (num_layers == 3) {
output_model = build_nn_model_3_layers(selected_train_df, selected_spec, batch_normalization, dropout_rate)
}
if (num_layers == 5) {
output_model = build_nn_model_5_layers(selected_train_df, selected_spec, batch_normalization, dropout_rate)
}
return (output_model)
}
grid_search_nn_model_generaL_optimizer <- function(selected_model, model_train_df, dropout_rates, num_layers,
epochs, batch_sizes, optimizer, patience_list,  verbose) {
selected_spec <- feature_spec(model_train_df, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scaler_standard()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
best_MAE <- Inf
best_model <- NA
best_history <- NA
batch_normalizations <- list(T, F)
for (dropout_rate in dropout_rates) {
for (batch_normalization in batch_normalizations) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
selected_model = build_model(model_train_df, selected_spec, num_layers, batch_normalization, dropout_rate)
print(paste(">Batch size", batch_size))
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience + 10)
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[mae_list_length]]
print(paste("> MAE of new model", new_mae))
#print(paste(">MAE of new model", new_mae))
#print(paste(">val MAE history", new_history$metrics$val_mean_absolute_error ))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
}
}
}
}
}
return (list(best_model, best_history))
}
grid_search_nn_model_generaL_optimizer <- function(model_train_df, dropout_rates, num_layers,
epochs, batch_sizes, optimizer, patience_list,  verbose) {
selected_spec <- feature_spec(model_train_df, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scaler_standard()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
best_MAE <- Inf
best_model <- NA
best_history <- NA
batch_normalizations <- list(T, F)
for (dropout_rate in dropout_rates) {
for (batch_normalization in batch_normalizations) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
selected_model = build_model(model_train_df, selected_spec, num_layers, batch_normalization, dropout_rate)
print(paste(">Batch size", batch_size))
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience + 10)
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[mae_list_length]]
print(paste("> MAE of new model", new_mae))
#print(paste(">MAE of new model", new_mae))
#print(paste(">val MAE history", new_history$metrics$val_mean_absolute_error ))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
}
}
}
}
}
return (list(best_model, best_history))
}
best_model_nn_3_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced,
dropout_rates = list(0.1, 0.3, 0.4),
num_layers = 3,
batch_sizes = list(500, 1000, 7000),
epochs = list(200),
optimizer = sgd_opt,
patience_list = list(5, 20,25, 40, 50),
verbose = 0
)
grid_search_nn_model_generaL_optimizer <- function(model_train_df, dropout_rates, num_layers,
epochs, batch_sizes, optimizer, patience_list,  verbose) {
selected_spec <- feature_spec(model_train_df, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scaler_standard()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
best_MAE <- Inf
best_model <- NA
best_history <- NA
batch_normalizations <- list(T, F)
for (dropout_rate in dropout_rates) {
for (batch_normalization in batch_normalizations) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
selected_model = build_model(model_train_df, selected_spec, num_layers, batch_normalization, dropout_rate)
print(paste(">Batch size", batch_size))
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience + 10)
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epochs,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[mae_list_length]]
print(paste("> MAE of new model", new_mae))
#print(paste(">MAE of new model", new_mae))
#print(paste(">val MAE history", new_history$metrics$val_mean_absolute_error ))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
}
}
}
}
}
return (list(best_model, best_history))
}
best_model_nn_3_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced,
dropout_rates = list(0.1, 0.3, 0.4),
num_layers = 3,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
optimizer = sgd_opt,
patience_list = list(5, 20,25, 40, 50),
verbose = 0
)
best_model_nn_3_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced,
dropout_rates = list(0.1, 0.3, 0.4),
num_layers = 3,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
optimizer = sgd_opt,
patience_list = list(5, 20,25, 40, 50),
verbose = 0
)
predictions_3_nn_model[ , 1]
postResample(predictions_3_nn_model[ , 1], test_df_reduced$retx)
predictions_3_nn_model <- best_model_nn_3_layer_test[[1]] %>% predict(test_df_reduced %>% dplyr::select(-retx))
predictions_3_nn_model[ , 1]
postResample(predictions_3_nn_model[ , 1], test_df_reduced$retx)
build_nn_model_3_layers <- function(selected_train_df, selected_spec, batch_normalization, dropout_rate) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(dropout_rate)
if (batch_normalization) output %<>% layer_batch_normalization()
output  %<>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(dropout_rate)
if (batch_normalization) output %<>% layer_batch_normalization()
output %<>%
layer_dense(units = 9)
model <- keras_model(input, output)
return(model)
}
predictions_3_nn_model <- best_model_nn_3_layer_test[[1]] %>% predict(test_df_reduced %>% dplyr::select(-retx, -permno))
predictions_3_nn_model[ , 1]
postResample(predictions_3_nn_model[ , 1], test_df_reduced$retx)
grid_search_nn_model_generaL_optimizer <- function(model_train_df, dropout_rates, num_layers,
epochs, batch_sizes, optimizer, patience_list,  verbose) {
selected_spec <- feature_spec(model_train_df, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scaler_standard()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
best_MAE <- Inf
best_model <- NA
best_history <- NA
batch_normalizations <- list(T, F)
for (dropout_rate in dropout_rates) {
for (batch_normalization in batch_normalizations) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
selected_model = build_model(model_train_df, selected_spec, num_layers, batch_normalization, dropout_rate)
print(paste(">Batch size", batch_size))
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience + 10)
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epochs,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[mae_list_length]]
print(paste("> MAE of new model", new_mae))
#print(paste(">MAE of new model", new_mae))
#print(paste(">val MAE history", new_history$metrics$val_mean_absolute_error ))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
}
}
}
}
}
return (list(best_model, best_history))
}
best_model_nn_3_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced,
dropout_rates = list(0.1, 0.3, 0.4),
num_layers = 3,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
optimizer = sgd_opt,
patience_list = list(1, 2, 5, 20,25, 40, 50),
verbose = 0
)
grid_search_nn_model_generaL_optimizer <- function(model_train_df, dropout_rates, num_layers,
epochs, batch_sizes, optimizer, patience_list,  verbose) {
selected_spec <- feature_spec(model_train_df, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scaler_standard()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
best_MAE <- Inf
best_model <- NA
best_history <- NA
batch_normalizations <- list(T, F)
for (dropout_rate in dropout_rates) {
for (batch_normalization in batch_normalizations) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
selected_model = build_model(model_train_df, selected_spec, num_layers, batch_normalization, dropout_rate)
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience + 10)
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epochs,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[mae_list_length]]
print(paste("> MAE of new model", new_mae))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
print(paste(">Batch size", batch_size))
print(paste(">dropout_rate", dropout_rate))
print(paste("> Patience", patience))
print(paste(">Batch normalization", batch_normalization))
}
}
}
}
}
return (list(best_model, best_history))
}
best_model_nn_3_layer_test <- grid_search_nn_model_generaL_optimizer(train_df_reduced,
dropout_rates = list(0, 0.1, 0.3, 0.4),
num_layers = 3,
batch_sizes = list(500, 1000, 7000),
epochs = 200,
optimizer = sgd_opt,
patience_list = list(1, 2, 5, 20,25, 40, 50),
verbose = 0
)
predictions_3_nn_model <- best_model_nn_3_layer_test[[1]] %>% predict(test_df_reduced %>% dplyr::select(-retx, -permno))
predictions_3_nn_model[ , 1]
postResample(predictions_3_nn_model[ , 1], test_df_reduced$retx)
