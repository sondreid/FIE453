multi_hidden_layer_model %>% summary()
keras_nn <- caret::train(retx ~ .,
data       = train_df_all,
preProcess = c("center", "scale"),
trControl  = train_control_nn,
tuneGrid   = keras_nn_grid,
metric     = "MAE",
verbose = T,
allowParalell = T,
method     = "keras")
train_control_nn <- trainControl(verboseIter = T,
savePredictions = T,
summaryFunction = defaultSummary)
keras_nn <- caret::train(retx ~ .,
data       = train_df_all,
preProcess = c("center", "scale"),
trControl  = train_control_nn,
tuneGrid   = keras_nn_grid,
metric     = "MAE",
verbose = T,
allowParalell = T,
method     = "mlpKerasDecay")
keras_nn_grid <-expand.grid(size = c(62,100), # According to the geometric pyramid rule (Masters, 1993),
lambda = c(0.001, 0), #Regularization rate
batch_size = 200,
lr = c(0.1, 0.05),
rho = c(0.001, 0.1), # Weight decay
decay= c(0.001, 0), # Learning rate decay
activation = "relu")
train_control_nn <- trainControl(verboseIter = T,
savePredictions = T,
summaryFunction = defaultSummary)
keras_nn <- caret::train(retx ~ .,
data       = train_df_all,
preProcess = c("center", "scale"),
trControl  = train_control_nn,
tuneGrid   = keras_nn_grid,
metric     = "MAE",
verbose = T,
allowParalell = T,
method     = "mlpKerasDecay")
keras_nn_grid <-expand.grid(size = c(62,100), # According to the geometric pyramid rule (Masters, 1993),
lambda = c(0.001, 0), #Regularization rate
batch_size = 500,
lr = c(0.1, 0.05),
rho = c(0.001, 0.1), # Weight decay
decay= c(0.001, 0), # Learning rate decay
activation = "relu")
cl <- makePSOCKcluster(2) # Use most cores, or specify
registerDoParallel(cl)
train_control_nn <- trainControl(verboseIter = F,
savePredictions = T,
summaryFunction = defaultSummary)
keras_nn <- caret::train(retx ~ .,
data       = train_df_all,
preProcess = c("center", "scale"),
trControl  = train_control_nn,
tuneGrid   = keras_nn_grid,
metric     = "MAE",
verbose = F,
allowParalell = T,
method     = "mlpKerasDecay")
stopCluster() # Stop previous cluster
################################################################################
########################### Pre-processing #####################################
################################################################################
# Candidates:
# Libraries --------------------------------------------------------------------
library(tidyverse)
library(magrittr)
library(tidymodels)
library(randomForest)
library(caret)
library(doParallel)
library(MLmetrics)
library(gbm)
library(PerformanceAnalytics)
library(kableExtra)
library(knitr)
library(monomvn)
library(kableExtra)
library(lubridate)
library(kknn)
#library(RSNNS)
library(nnet)
# Set WD -----------------------------------------------------------------------
#setwd("~/OneDrive - Norges HandelshÃ¸yskole/MASTER/FIE453/FinalExam/FIE453/Final Paper")
# Load and read data -----------------------------------------------------------
load("data/merged.Rdata")
company_names_df <- read.csv(file = "descriptions/names.csv")
feature_names_df <- read.delim(file = "descriptions/compustat-fields.txt")
company_names_df %<>% rename_with(tolower) %>% mutate(date = ymd(date))
feature_names_df %<>% rename_with(tolower)
merged %<>% rename_with(tolower)
get_company_name <- function(input_permno) {
#'
#'@description: Returns the name of a company based on its company identification number
company_name <- company_names_df %>%
filter(permno == input_permno)
# If several names are registered. Pick the most recent
company_name %<>% arrange(desc(date))
return( company_name$comnam[1])
}
## Variables that cannot be inclduded with dependent variable RETX
## REMEMBER TO INCLUDE DATE WHEN TIME SPLITTING
# Irrelevant features ----------------------------------------------------------
# Variables that cannot be included with dependent variable RETX
excluded_variables <- c("ret",
"prc",         # Price should maybe be allowed
"vwretd",      # vwretd: market excess return
"datadate",
"date",        # Remove all date related variables
"datafqtr",
"fyearq",
"fyr",
"fqtr",
"datacqtr")
merged %<>% dplyr::select(-excluded_variables)
# Data reduction ---------------------------------------------------------------
get_subset_of_companies <-function(df, number_of_companies) {
#' @description:              To reduce run time, we want to reduce the
#'                            number of companies,(for variable selection
#'                            purposes)
#'
#' @param df                  The dataframe to be split
#' @param number_of_companies The number of speakers to be retained
#' @return                    A dataframe of fewer companies
set.seed(123)
companies <- df$permno %>% unique()
subset_of_companies <- companies %>%
sample(x = ., size = number_of_companies)
return(df %>% filter(permno %in% subset_of_companies))
}
get_subset_of_companies_ratio <-function(df, ratio) {
#' @Description:              To reduce run time, we want to reduce the
#'                            number of companies, (for variable selection
#'                            purposes)
#'
#' @param df                  The dataframe to be split
#' @param number_of_companies The number of speakers to be retained
#' @return:                   A dataframe of fewer companies
set.seed(123)
companies <- df$permno %>% unique()
number_of_companies <- companies %>% length()
subset_of_companies <- companies %>%
sample(x = ., size = as.integer(number_of_companies*ratio))
return(df %>% filter(permno %in% subset_of_companies))
}
# Feature selection functions --------------------------------------------------
remove_cols_only_zero_and_NA <- function(df, print_removed_cols = F) {
#' @description              Function that removes columns containing only
#'                           zeros and NAs
#'
#' @param df                 Passing a data frame
#' @param print_removed_cols True if user want to print removed columns
#' @return                   Data frame without columns that containing only
#'                           zeros and NAs
cols <- df %>%
apply(MARGIN = 2,
function(x) (sum(x==0, na.rm = T) + sum(is.na(x)))/length(x))
cols <- cols[cols == 1] %>%
as.data.frame() %>%
rownames()
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return (df %>% dplyr::select(-cols))
}
remove_NA <- function(df, ratio, print_removed_cols = F){
#' @description              Function that removes columns containing NAs
#'                           beyond a given ratio
#'
#' @param df                 Passing a data frame
#' @param ratio              Passing a upper limit NA ratio
#' @param print_removed_cols True if user want to print removed columns
#' @return                   Data frame without columns containing NAs
#'                           beyond given ratio
cols <- df %>%
apply(MARGIN = 2,
function(x) sum(is.na(x))/length(x))
cols <- cols[cols >= ratio] %>%
as.data.frame() %>%
rownames()
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% dplyr::select(-cols))
}
remove_nzv <- function(df, print_removed_cols = F){
#' @description              Function that removes near zero variance
#'                           columns
#'
#' @param df                 Passing a data frame
#' @param print_removed_cols True if user want to print removed columns
#' @return                   Data frame without columns near zero variance
#'                           columns
rec <- recipe(retx ~ .,
data = df)
cols <- (rec %>%
step_nzv(all_predictors()) %>%
prep(df) %>%
tidy(number = 1))$terms
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% dplyr::select(-cols))
}
remove_hcv <- function(df, threshold = 0.9, print_removed_cols = F){
#' @description              Function that removes highly correlated
#'                           features
#'
#' @param df                 Passing a data frame
#' @param treshold           Correlation beneath this threshold
#' @param print_removed_cols True if user want to print removed columns
#' @return                   Data frame without highly correlated features
numeric_cols <- df %>%
lapply(is.numeric) %>%
unlist()
rec <- recipe(retx ~ .,
data = df[numeric_cols])
cols <- (rec %>%
step_corr(all_predictors(),
threshold = threshold) %>%
prep(df[numeric_cols]) %>%
tidy(number = 1))$terms
if(print_removed_cols) cat("Columns removed: ", cols, "\n\n")
return(df %>% dplyr::select(-cols))
}
replace_NA_with_mean <- function(df, print_replaced_cols = F){
#' @description               Function that replaces NA with column means
#'
#' @param df                  Passing a data frame
#' @param print_replaced_cols True if user want to print replaced columns
#' @return                    Data frame NA-replaced column means
na_cols <- df %>%
apply(MARGIN = 2,
function(x) any(is.na(x)))
numeric_cols <- df[na_cols] %>%
lapply(is.numeric) %>%
unlist()
col_means <- df[na_cols] %>%
colMeans(na.rm = T)
col_names <- col_means %>%
names()
for (col in col_names){
df[col] <- df[col][[1]] %>%
replace_na(col_means[col])
}
if(print_replaced_cols) cat("Columns replaced: ", col_names, "\n\n")
return(df)
}
remove_NA_rows <- function(df) {
#' @description Function that removes any rows with one or more NA's
#'
#' @param df    Passing a data frame
#' @return      Data frame NA filtered rows
return(df %>% filter(across(everything(), ~ !is.na(.x))) )
}
perform_train_test_split <- function(df, train_ratio = 0.8) {
#' @description Ensures an equal amount of companies in each set
#'
#' @param df    The dataframe to be split
#' @param ratio Ratio of training data, (validation and test set to equal
#'              length)
#' @return      A list of three data frames: training, validation and
#'              test sets
set.seed(123)
all_companies <- df$permno %>% unique()
train_indices <- sample(1:length(all_companies),
floor(length(all_companies) * train_ratio))
train_companies <- all_companies[train_indices]
test_companies <- all_companies[-train_indices]
train_sample <- df %>% filter(permno %in% train_companies)
test_sample  <- df %>% filter(permno %in% test_companies)
return (list(train_sample, test_sample))
}
find_company_observations <- function(df, minimum_observations) {
#' @description                 Finds companies that have less than a
#'                              minimum amount of observations
#'
#' @param df                    Passing a data frame
#' @param minimum_observations  Passing minimum observation limit
#' @return                      A data frame
all_companies <- df$permno %>%
unique()
df %<>% group_by(permno) %>%
summarise(count = n()) %>%
ungroup() %>%
filter(count < minimum_observations) %>%
arrange(desc(count))
return(df)
}
expanded_summary  <- function(data, lev = NULL, model = NULL){
#' @description
#'
#' @param data
#' @param lev
#' @param model
a1 <- defaultSummary(data, lev, model)
c1 <- prSummary(data, lev, model)
out <- c(a1, b1, c1)
out
}
# Train Control
train_control <- trainControl(method = "cv",
number = 10,
verboseIter = T,
savePredictions = T,
summaryFunction = defaultSummary)
################################################################################
########################## Train and Test Split ################################
################################################################################
######### Dataframe with all companies using only variance and correlation filter#############
# Load all
load(file = "cached_data/train_test.Rdata")
df_all <- merged %>%
remove_cols_only_zero_and_NA(print_removed_cols = T) %>%
remove_NA(0.2, print_removed_cols = T) %>%
remove_nzv(print_removed_cols = T) %>%
remove_hcv(0.9, print_removed_cols = T) %>%
remove_NA_rows() # Remove rows with NA's
# Train-Test-Split
train_test <- perform_train_test_split(df_all,
train_ratio = 0.8)                       # Split into train and test set with seperate sets of companies
train_df_all <- train_test[[1]]
test_df_all <- train_test[[2]]
train_df_all %<>% dplyr::select(-permno) # Remove company numbers from training
low_observation_count_companies <- find_company_observations(test_df_all, 60)
test_df_all %<>% anti_join(low_observation_count_companies)                        # Cut companies with fewer than 50 observations (they cannot be reliably predicted)
rm(merged, df_all) # Remove large datasets from memory
save(train_df_all, test_df_all, file = "cached_data/train_test.Rdata")
library(tensorflow)
library(keras)
library(reticulate)
conda_python(envname = "r-reticulate")
tensorflow::use_condaenv("r-reticulate")
keras_nn_grid <-expand.grid(size = c(62,100), # According to the geometric pyramid rule (Masters, 1993),
lambda = c(0.001, 0), #Regularization rate
batch_size = 500,
lr = c(0.1, 0.05),
rho = c(0.001, 0.1), # Weight decay
decay= c(0.001, 0), # Learning rate decay
activation = "relu")
train_control_nn <- trainControl(verboseIter = F,
savePredictions = T,
summaryFunction = defaultSummary)
keras_nn <- caret::train(retx ~ .,
data       = train_df_all,
preProcess = c("center", "scale"),
trControl  = train_control_nn,
tuneGrid   = keras_nn_grid,
metric     = "MAE",
verbose = F,
allowParalell = T,
method     = "mlpKerasDecay")
model <- keras_model_sequential() %>%
layer_dense_features(dense_features(spec_prep)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
library(keras)
model <- keras_model_sequential() %>%
layer_dense_features(dense_features(spec_prep)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
spec <- feature_spec(train_df_all, retx ~ .)%>%
step_numeric_column(
all_numeric(),
normalizer_fn = scaler_standard()
)
step_numeric_column
??step_numeric_column
spec <- feature_spec(train_df_all, retx ~ .)
tensorflow::feature_spec
input <- layer_input_from_dataset(train_df_all %>% select(-retx))
output <- input %>%
layer_dense_features(dense_features(spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 8)
layer_dense_features
layer_dense_features
dense_features
spec <- feature_spec(train_df_all, retx ~ . ) %>%
step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>%
fit()
??step_numeric_column
library(tfdatasets)
install.packages("tfdatasets")
library(reticulate)
library(tfdatasets)
??step_numeric_column
spec <- feature_spec(train_df_all, retx ~ . ) %>%
step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>%
fit()
input <- layer_input_from_dataset(train_df_all %>% select(-retx))
train_df_all %>% select(-retx)
input <- layer_input_from_dataset(train_df_all %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 8,  activation = "relu")
model <- keras_model(input, output)
model %>%
compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
history <- model %>% fit(
x = train_df_all %>% dplyr::select(-label),
y = train_df_all$retx,
epochs = 500,
validation_split = 0.2,
verbose = 0,
callbacks = list(print_dot_callback)
)
history <- model %>% fit(
x = train_df_all %>% dplyr::select(-retx),
y = train_df_all$retx,
epochs = 500,
validation_split = 0.2,
verbose = 0,
callbacks = list(print_dot_callback)
)
build_model <- function() {
input <- layer_input_from_dataset(train_df_all %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 8,  activation = "relu")
model <- keras_model(input, output)
model %>%
compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
return (model)
}
model <- build_model()
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat("\n")
cat(".")
}
)
history <- model %>% fit(
x = train_df_all %>% dplyr::select(-retx),
y = train_df_all$retx,
epochs = 500,
validation_split = 0.2,
verbose = 0,
callbacks = list(print_dot_callback)
)
model <- build_model()
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat("\n")
cat(".")
}
)
history <- model %>% fit(
x = train_df_all %>% dplyr::select(-retx),
y = train_df_all$retx,
epochs = 20,
batch_size = 500,
validation_split = 0.2,
verbose = 1,
callbacks = list(print_dot_callback)
)
# Predict
test_predictions <- model %>% predict(test_df_all %>% dplyr::select(-retx))
test_predictions[ , 1]
test_df_all$retx
c(loss, mae) %<-% (model %>% evaluate(test_df_all %>% dplyr::select(-retx), test_df_all$retx, verbose = 0))
paste0("Mean absolute error on test set: $", sprintf("%.2f", mae * 1000))
tensorflow::SGD
??SGD
spec <- feature_spec(train_df_all, retx ~ . ) %>%
step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>%
fit()
build_model <- function() {
input <- layer_input_from_dataset(train_df_all %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 8,  activation = "relu")
model <- keras_model(input, output)
model %>%
compile(
loss = loss_binary_crossentropy,
optimizer = optimizer_sgd(
learning_rate = 0.01,
momentum = 0.001,
decay = c(0.001, 0)),
metrics = list("mean_absolute_error")
)
return (model)
}
model <- build_model()
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat("\n")
cat(".")
}
)
history <- model %>% fit(
x = train_df_all %>% dplyr::select(-retx),
y = train_df_all$retx,
epochs = 40,
batch_size = 200,
validation_split = 0.2,
verbose = 1,
callbacks = list(print_dot_callback)
)
