new_early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience)
new_model <- selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
history <- new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_history <- history
best_model <- new_model
best_MAE <- mae
}
}
}
}
return (list(best_model, best_history))
}
best_model_nn_3_layer_adam <- grid_search_nn_model_generaL_optimizer(nn_model_3_layers_reduced, train_df_reduced, test_df_reduced,
batch_sizes = list(100, 500, 7000),
epochs = list(700),
optimizer = adam_opt,
patience_list = list(5, 15, 25, 50),
verbose = 0
)
best_model_nn_3_layer_adam
nn_model_1_layer
nn_model_1_layer <- build_nn_model_1_layer(train_df, spec)
nn_model_3_layers <- build_nn_model_3_layers(train_df, spec)
nn_model_5_layers <- build_nn_model_5_layers(train_df, spec)
best_model_nn_3_layer_adam[[2]]
best_model_nn_3_layer_adam[[2]] %>% plot()
### Load keras and tensorflow ###
library(tensorflow)
library(keras)
library(tfdatasets)
library(reticulate)
set_random_seed (42, disable_gpu = FALSE) # Set seed for reproducability, both tensorflow and R native seed
conda_python(envname = "r-reticulate") # Create miniconda enviroment (if not already done)
tensorflow::use_condaenv("r-reticulate") # Specify enviroment to tensorflow
################################################################################
########################## Train and Test Split ################################
################################################################################
######### Dataframe with all companies using only variance and correlation filter#############
# Load all
load(file = "cached_data/train_test.Rdata")
# Or run the code in preprocessing
################################################################################
######################### Load or run models ###################################
################################################################################
# In order to save run time one can choose to load the model results
load(file = "models/models.Rdata")
make_0_benchmark <- function(selected_test_df) {
#' Makes a zero benchmark to compare models
#' @
benchmark_0 <- postResample(rep(0, nrow(selected_test_df)), selected_test_df$retx)
return (benchmark_0)
}
# Neural network using specified number of layers  ---------------------------------------
spec <- feature_spec(train_df, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scaler_standard()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
spec_reduced <- feature_spec(train_df_reduced, retx ~ . ) %>%
step_numeric_column(all_numeric(), -costat, normalizer_fn = scaler_standard()) %>% # Scale numeric features
step_categorical_column_with_vocabulary_list(costat) %>%  # non-numeric variables
fit()
print_dot_callback <- callback_lambda(
#' Simplified callback, showing dots instead of full loss/validation error plots
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat("\n")
cat(".")
}
)
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
build_nn_model_5_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dropout(0.3) %>%
layer_batch_normalization() %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dropout(0.3) %>%
layer_batch_normalization() %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dropout(0.3) %>%
layer_batch_normalization() %>%
layer_dense(units = 50, activation = "relu") %>%
layer_dropout(0.3) %>%
layer_batch_normalization() %>%
layer_dense(units = 50)
model <- keras_model(input, output)
return (model)
}
build_nn_model_3_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dropout(0.3) %>%
layer_batch_normalization() %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dropout(0.3) %>%
layer_batch_normalization() %>%
layer_dense(units = 50)
model <- keras_model(input, output)
return(model)
}
build_nn_model_1_layer <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32)
model <- keras_model(input, output)
return(model)
}
grid_search_nn_model <- function(selected_model, model_train_df, model_test_df, learning_rates, momentums,
epochs, batch_sizes,
patience_list, verbose) {
best_MAE <- Inf
best_model <- NA
best_history <- NA
for (lr in learning_rates) {
for (momentum in momentums) {
for (epoch in epochs) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
new_early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience)
new_model <- selected_model %>%
compile(
loss = "mse",
optimizer = optimizer_sgd(
learning_rate =lr,
momentum = momentum),
metrics = list("mean_absolute_error"))
history <- new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
c(loss, mae) %<-% (new_model %>% evaluate(model_test_df %>% dplyr::select(-retx), model_test_df$retx, verbose = 0))
if (mae < best_MAE) {
best_history <- history
best_model <- new_model
best_MAE <- mae
}
}
}
}
}
}
return (list(best_model, best_history))
}
grid_search_nn_model_generaL_optimizer <- function(selected_model, model_train_df,
epochs, batch_sizes, optimizer, patience_list,  verbose) {
best_MAE <- Inf
best_model <- NA
best_history <- NA
for (epoch in epochs) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience +10 )
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[mae_list_length]]
print(paste("> MAE of new model", new_mae))
#print(paste(">MAE of new model", new_mae))
#print(paste(">val MAE history", new_history$metrics$val_mean_absolute_error ))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
}
}
}
}
return (list(best_model, best_history))
}
# Libraries --------------------------------------------------------------------
library(tidyverse)
library(magrittr)
library(tidymodels)
library(randomForest)
library(caret)
library(doParallel)
library(MLmetrics)
library(gbm)
library(PerformanceAnalytics)
library(kableExtra)
library(knitr)
library(monomvn)
library(kableExtra)
library(lubridate)
library(kknn)
library(nnet)
set.seed(1)
nn_model_1_layer_reduced <- build_nn_model_1_layer(train_df_reduced, spec_reduced)
nn_model_3_layers_reduced <- build_nn_model_3_layers(train_df_reduced, spec_reduced)
nn_model_5_layers_reduced <- build_nn_model_5_layers(train_df_reduced, spec_reduced)
nn_model_1_layer <- build_nn_model_1_layer(train_df, spec)
nn_model_3_layers <- build_nn_model_3_layers(train_df, spec)
nn_model_5_layers <- build_nn_model_5_layers(train_df, spec)
best_model_nn_1_layer_all <- grid_search_nn_model_generaL_optimizer(nn_model_1_layer, train_df, test_df,
batch_sizes = list(100, 500, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(5, 15, 25, 50),
verbose = 0
)
best_model_nn_3_layers_all <- grid_search_nn_model_generaL_optimizer(nn_model_3_layers, train_df, test_df,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(12,20, 25, 50),
verbose = 0
)
best_model_nn_1_layer_all <- grid_search_nn_model_generaL_optimizer(nn_model_1_layer, train_df,
batch_sizes = list(100, 500, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(5, 15, 25, 50),
verbose = 0
)
best_model_nn_3_layers_all <- grid_search_nn_model_generaL_optimizer(nn_model_3_layers, train_df,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(12,20, 25, 50),
verbose = 0
)
adam_opt = optimizer_adam()
best_model_nn_1_layer_all <- grid_search_nn_model_generaL_optimizer(nn_model_1_layer, train_df,
batch_sizes = list(100, 500, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(5, 15, 25, 50),
verbose = 0
)
best_model_nn_3_layers_all <- grid_search_nn_model_generaL_optimizer(nn_model_3_layers, train_df,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(12,20, 25, 50),
verbose = 0
)
best_model_nn_5_layers_all <- grid_search_nn_model_generaL_optimizer(nn_model_5_layers, train_df,
batch_sizes = list(100, 500, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(12,20, 25, 50),
verbose = 0
)
build_nn_model_2_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_batch_normalization() %>%
layer_dense(units = 16)
model <- keras_model(input, output)
return(model)
}
nn_model_2_layers <- build_nn_model_2_layers(train_df, spec)
build_nn_model_2_layers <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_batch_normalization() %>%
layer_dense(units = 16)
model <- keras_model(input, output)
return(model)
}
build_nn_model_1_layer <- function(selected_train_df, selected_spec) {
input <- layer_input_from_dataset(selected_train_df %>% dplyr::select(-retx))
output <- input %>%
layer_dense_features(dense_features(selected_spec)) %>%
layer_dense(units = 32)
model <- keras_model(input, output)
return(model)
}
nn_model_1_layer <- build_nn_model_1_layer(train_df, spec)
nn_model_2_layers <- build_nn_model_2_layers(train_df, spec)
nn_model_3_layers <- build_nn_model_3_layers(train_df, spec)
nn_model_5_layers <- build_nn_model_5_layers(train_df, spec)
best_model_nn_1_layer_all <- grid_search_nn_model_generaL_optimizer(nn_model_1_layer, train_df,
batch_sizes = list(100, 300, 500, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(1, 5, 15, 25, 50),
verbose = 0
)
best_model_nn_2_layer_all <- grid_search_nn_model_generaL_optimizer(nn_model_2_layers, train_df,
batch_sizes = list(100, 300, 500, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(1, 5, 15, 25, 50),
verbose = 0
)
grid_search_nn_model_generaL_optimizer <- function(selected_model, model_train_df,
epochs, batch_sizes, optimizer, patience_list,  verbose) {
best_MAE <- Inf
best_model <- NA
best_history <- NA
for (epoch in epochs) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience +10 )
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[1]]
print(paste("> MAE of new model", new_mae))
#print(paste(">MAE of new model", new_mae))
#print(paste(">val MAE history", new_history$metrics$val_mean_absolute_error ))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
}
}
}
}
return (list(best_model, best_history))
}
grid_search_nn_model_generaL_optimizer <- function(selected_model, model_train_df,
epochs, batch_sizes, optimizer, patience_list,  verbose) {
best_MAE <- Inf
best_model <- NA
best_history <- NA
for (epoch in epochs) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience +10 )
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, reduce_lr, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[1]]
print(paste("> MAE of new model", new_mae))
#print(paste(">MAE of new model", new_mae))
#print(paste(">val MAE history", new_history$metrics$val_mean_absolute_error ))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
}
}
}
}
return (list(best_model, best_history))
}
best_model_nn_1_layer_all <- grid_search_nn_model_generaL_optimizer(nn_model_1_layer, train_df,
batch_sizes = list(100, 300, 500, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(1, 5, 15, 25, 50),
verbose = 0
)
best_model_nn_2_layer_all <- grid_search_nn_model_generaL_optimizer(nn_model_2_layers, train_df,
batch_sizes = list(100, 300, 500, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(1, 5, 15, 25, 50),
verbose = 0
)
best_model_nn_1_layer_all[[1]]
best_model_nn_1_layer_all
predictions_1_nn_model <- best_model_nn_1_layer_all[[1]] %>% predict(test_df %>% dplyr::select(-retx))
predictions_1_nn_model[ , 1]
postResample(predictions_1_nn_model[ , 1], test_df$retx)
grid_search_nn_model_generaL_optimizer <- function(selected_model, model_train_df,
epochs, batch_sizes, optimizer, patience_list,  verbose) {
best_MAE <- Inf
best_model <- NA
best_history <- NA
for (epoch in epochs) {
for (batch_size in batch_sizes) {
for (patience in patience_list) {
#reduce_lr = callback_reduce_lr_on_plateau(monitor = "val_loss", patience = patience)
new_early_stop = callback_early_stopping(monitor = "val_loss", patience = patience)
new_model = selected_model %>%
compile(
loss = "mse",
optimizer = optimizer,
metrics = list("mean_absolute_error"))
new_history = new_model %>%
fit(
x = model_train_df %>% dplyr::select(-retx),
y = model_train_df$retx,
epochs = epoch,
batch_size = batch_size,
validation_split = 0.2,
verbose = verbose,
callbacks = list(print_dot_callback, new_early_stop) #Print simplified dots, and stop learning when validation improvements stalls
)
mae_list_length =  new_history$metrics$val_mean_absolute_error %>% length()
new_mae =  new_history$metrics$val_mean_absolute_error[[1]]
print(paste("> MAE of new model", new_mae))
#print(paste(">MAE of new model", new_mae))
#print(paste(">val MAE history", new_history$metrics$val_mean_absolute_error ))
if (new_mae < best_MAE) {
best_history <- new_history
best_model <- new_model
best_MAE <- new_mae
print(paste(">New best model. MAE of new model", best_MAE))
}
}
}
}
return (list(best_model, best_history))
}
nn_model_1_layer_reduced <- build_nn_model_1_layer(train_df_reduced, spec_reduced)
nn_model_3_layers_reduced <- build_nn_model_3_layers(train_df_reduced, spec_reduced)
nn_model_5_layers_reduced <- build_nn_model_5_layers(train_df_reduced, spec_reduced)
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_1_layer_reduced, train_df_reduced,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(20,25, 40, 50),
verbose = 0
)
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_1_layer_reduced, train_df_reduced,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(1, 2, 5, ,7, 10, 20,25, 40, 50),
verbose = 0
)
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_1_layer_reduced, train_df_reduced,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(1, 2, 5, 7, 10, 20,25, 40, 50),
verbose = 0
)
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_1_layer_reduced, train_df_reduced,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(1, 2, 5, 7, 10, 20,25, 40, 50),
verbose = 0
)
best_model_nn_1_layer_test <- grid_search_nn_model_generaL_optimizer(nn_model_1_layer_reduced, train_df_reduced,
batch_sizes = list(100, 500, 1000, 7000),
epochs = list(200),
optimizer = adam_opt,
patience_list = list(1, 2, 5, 7, 10, 20,25, 40, 50),
verbose = 1
)
